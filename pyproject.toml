[tool.poetry]
name = "llm_metrics"
version = "0.1.0"
description = "A Python library for reference-based metrics to compare generated text with ground truth text"
authors = ["niting <niting1209@gmail.com>"]
license = "MIT"
readme = "README.md"
repository = "https://github.com/ai4society/GenAIResultsComparator"
keywords = ["nlp", "metrics", "llm", "evaluation"]

[tool.poetry.dependencies]
python = ">=3.10"
numpy = "^2.1.2"
scipy = "^1.14.1"
nltk = "^3.9.1"
pandas = "^2.2.3"
pytest = "^8.3.3"
rouge-score = "^0.1.2"
bert-score = "^0.3.13"
sentence-transformers = "^3.2.0"
scikit-learn = "^1.5.2"
levenshtein = "^0.26.0"

[tool.poetry.group.dev.dependencies]
pytest = "^8.3.3"
black = "^24.10.0"
flake8 = "^7.1.1"
mypy = "^1.12.0"
hypothesis = "^6.115.2"
sphinx = "^8.1.3"
pre-commit = "^4.0.1"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.black]
line-length = 100
target-version = ['py38']

[tool.isort]
profile = "black"
line_length = 100

[tool.mypy]
ignore_missing_imports = true
