[project]
name = "llm_metrics"
version = "0.1.0"
description = "A Python library for reference-based metrics to compare generated text with ground truth text"
authors = [{ name = "niting", email = "niting1209@gmail.com" }]
requires-python = ">=3.10, <3.13"
readme = "README.md"
license = "MIT"
keywords = [
    "nlp",
    "metrics",
    "llm",
    "evaluation",
]
dependencies = [
    "bert-score>=0.3.13",
    "jupyter>=1.1.1",
    "levenshtein>=0.27.1",
    "nltk>=3.9.1",
    "numpy>=2.2.5",
    "pandas>=2.2.3",
    "pytest>=8.3.5",
    "rouge-score>=0.1.2",
    "scikit-learn>=1.6.1",
    "scipy>=1.15.3",
    "sentence-transformers>=4.1.0",
]

[project.urls]
Repository = "https://github.com/ai4society/GenAIResultsComparator"

[project.optional-dependencies]
visualization = [
    "matplotlib>=3.10.3",
    "seaborn>=0.13.2",
]

[dependency-groups]
dev = [
    "black>=25.1.0",
    "flake8>=7.2.0",
    "hypothesis>=6.131.17",
    "mypy>=1.15.0",
    "pre-commit>=4.2.0",
    "pytest>=8.3.5",
    "sphinx>=8.1.3",
]

[tool.pytest.ini_options]
markers = [
    "bertscore: marker for BERTScore tests",
]
