[project]
name = "llm_metrics"
version = "0.1.0"
description = "A Python library for reference-based metrics to compare generated text with ground truth text"
authors = [{ name = "niting", email = "niting1209@gmail.com" }]
requires-python = ">=3.10, <3.13"
readme = "README.md"
license = "MIT"
keywords = [
    "nlp",
    "metrics",
    "llm",
    "evaluation",
]

[project.urls]
Repository = "https://github.com/ai4society/GenAIResultsComparator"

dependencies = [
    "numpy>=2.1.2,<3",
    "scipy>=1.14.1,<2",
    "nltk>=3.9.1,<4",
    "pandas>=2.2.3,<3",
    "pytest>=8.3.3,<9",
    "rouge-score>=0.1.2,<0.2",
    "bert-score>=0.3.13,<0.4",
    "sentence-transformers>=3.2.0,<4",
    "scikit-learn>=1.5.2,<2",
    "levenshtein>=0.26.0,<0.27",
    "jupyter>=1.1.1,<2",
]

[project.optional-dependencies]
visualization = [
    "matplotlib>=3.7.0,<4",
    "seaborn>=0.13.2,<0.14",
]

[dependency-groups]
dev = [
    "pytest>=8.3.3,<9",
    "black>=24.10.0,<25",
    "flake8>=7.1.1,<8",
    "mypy>=1.12.0,<2",
    "hypothesis>=6.115.2,<7",
    "sphinx>=8.1.3,<9",
    "pre-commit>=4.0.1,<5",
]

[tool.black]
line-length = 100
target-version = ['py38']

[tool.isort]
profile = "black"
line_length = 100

[tool.mypy]
ignore_missing_imports = true

[tool.pytest.ini_options]
markers = [
    "bertscore: marker for BERTScore tests",
]
