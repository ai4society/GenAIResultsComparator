{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a142ad-a0f4-4704-a725-9a419a8d2db3",
   "metadata": {},
   "source": [
    "# Prompt-Response Alignment Score Example\n",
    "\n",
    "This notebook demonstrates how to use the Prompt-Response Alignment Metric to evaluate how well responses align with their prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b7f691-918a-4a65-a852-c25ac29366a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.llm_aware_metrics.code.alignment_score import PromptAlignmentMetric\n",
    "from llm_metrics.semantic_similarity_metrics import BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a67d2f-8f0c-45e6-8b59-911e93b451c7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc7419-5033-45ce-9e6e-14399ada9d2c",
   "metadata": {},
   "source": [
    "## Example Data\n",
    "Example prompts and responses with varying degrees of alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd95bb05-f64a-4528-a2a2-7394feb97e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"What are the main causes of climate change?\"\n",
    "\n",
    "# Well-aligned response\n",
    "response1 = \"The main causes of climate change include greenhouse gas emissions from burning fossil fuels, deforestation, and industrial processes.\"\n",
    "\n",
    "# Partially aligned response\n",
    "response2 = \"Climate change is a serious issue. We need to reduce pollution and plant more trees.\"\n",
    "\n",
    "# Poorly aligned response\n",
    "response3 = \"The weather has been quite unusual lately. Yesterday it rained all day.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f2ca5-274f-4b4c-bf0a-f5948393b850",
   "metadata": {},
   "source": [
    "## Using the Alignment Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6c1675-f7c1-4212-bffb-bca83a421c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the metric\n",
    "base_metric = BERTScore(model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "alignment_metric = PromptAlignmentMetric(base_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ad7c6c4-3979-4247-b2ae-267b8062c9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment score for well-aligned responses:\n",
      "{'precision': 0.6514176527659098, 'recall': 0.5639405051867167, 'f1': 0.600588838259379}\n"
     ]
    }
   ],
   "source": [
    "# Compare well-aligned responses\n",
    "score1 = alignment_metric.calculate_with_prompt(\n",
    "    response1,\n",
    "    response2,\n",
    "    prompt1\n",
    ")\n",
    "\n",
    "print(f\"Alignment score for well-aligned responses:\\n{score1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c8b1b32-d05e-42cc-bc3c-eb7ca0cf1e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment score with poorly aligned response:\n",
      "{'precision': 0.5123771925767263, 'recall': 0.4340182642141978, 'f1': 0.4655380845069885}\n"
     ]
    }
   ],
   "source": [
    "# Compare with poorly aligned response\n",
    "score2 = alignment_metric.calculate_with_prompt(\n",
    "    response1,\n",
    "    response3,\n",
    "    prompt1\n",
    ")\n",
    "\n",
    "print(f\"Alignment score with poorly aligned response:\\n{score2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59decc0b-c789-43dd-be97-a187b46cde3d",
   "metadata": {},
   "source": [
    "## Analyzing Components of the Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78361c1-3f19-443f-bbc9-a98354128b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of well-aligned responses:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to dict.__format__",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse Similarity: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse_similarity\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnalysis of well-aligned responses:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m \u001B[43manalyze_alignment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAnalysis with poorly aligned response:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m analyze_alignment(response1, response3, prompt1)\n",
      "Cell \u001B[0;32mIn[13], line 8\u001B[0m, in \u001B[0;36manalyze_alignment\u001B[0;34m(response1, response2, prompt)\u001B[0m\n\u001B[1;32m      5\u001B[0m alignment2 \u001B[38;5;241m=\u001B[39m alignment_metric\u001B[38;5;241m.\u001B[39mcalculate_prompt_alignment(prompt, response2)\n\u001B[1;32m      6\u001B[0m response_similarity \u001B[38;5;241m=\u001B[39m base_metric\u001B[38;5;241m.\u001B[39mcalculate(response1, response2)\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse 1 Prompt Alignment: \u001B[39m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43malignment1\u001B[49m\u001B[38;5;132;43;01m:\u001B[39;49;00m\u001B[38;5;124;43m.3f\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse 2 Prompt Alignment: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00malignment2\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse Similarity: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse_similarity\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported format string passed to dict.__format__"
     ]
    }
   ],
   "source": [
    "# Let's break down the components of the alignment score\n",
    "def analyze_alignment(response1, response2, prompt):\n",
    "    # Get individual components\n",
    "    alignment1 = alignment_metric.calculate_prompt_alignment(prompt, response1)\n",
    "    alignment2 = alignment_metric.calculate_prompt_alignment(prompt, response2)\n",
    "    response_similarity = base_metric.calculate(response1, response2)\n",
    "    \n",
    "    print(f\"Response 1 Prompt Alignment: {alignment1:.3f}\")\n",
    "    print(f\"Response 2 Prompt Alignment: {alignment2:.3f}\")\n",
    "    print(f\"Response Similarity: {response_similarity:.3f}\")\n",
    "\n",
    "print(\"Analysis of well-aligned responses:\")\n",
    "analyze_alignment(response1, response2, prompt1)\n",
    "\n",
    "print(\"\\nAnalysis with poorly aligned response:\")\n",
    "analyze_alignment(response1, response3, prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c459823-9d66-4c1f-b982-1eeafab0abdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
