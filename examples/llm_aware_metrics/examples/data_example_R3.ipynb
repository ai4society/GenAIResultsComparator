{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8657abf8fd62e9",
   "metadata": {},
   "source": [
    "# Evaluating LLM Recipe Processing with LLM-Aware Metrics \n",
    "This notebook demonstrates how to use the LLM-aware metrics module to evaluate recipe processing tasks. We'll focus on:\n",
    "\n",
    "1. Loading and preprocessing recipe data\n",
    "2. Evaluating structural consistency\n",
    "3. Analyzing prompt aggregated similarity scores\n",
    "4. Comparing response quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0c947b031d88f",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a93b01e6567a79b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:31:49.264908Z",
     "start_time": "2025-01-22T16:31:45.024340Z"
    }
   },
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Import our custom metrics\n",
    "from llm_metrics.semantic_similarity_metrics import BERTScore\n",
    "from examples.llm_aware_metrics.code.prompt_aware import PromptAwareMetric\n",
    "from examples.llm_aware_metrics.code.schema_based import SchemaAwareMetric\n",
    "from examples.llm_aware_metrics.code.aggregated_similarity_score import AggregatedSimilarityMetric"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "879d1b17c063bddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:31:49.269153Z",
     "start_time": "2025-01-22T16:31:49.265929Z"
    }
   },
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "def load_recipe_data(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load recipe conversion data from JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return {\n",
    "        \"system_prompt\": data[\"system-prompt\"],\n",
    "        \"user_prompt\": data[\"prompt\"],\n",
    "        \"llm_response\": data[\"response\"],\n",
    "    }\n",
    "\n",
    "# Load our recipe data\n",
    "recipe_data = load_recipe_data('../../data/R3_conversion_1-shot-0.3.json')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Recipe Schema",
   "id": "315d32a20fb5e88c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:31:49.274761Z",
     "start_time": "2025-01-22T16:31:49.269901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the recipe schema\n",
    "RECIPE_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\n",
    "        \"recipe_name\",\n",
    "        \"macronutrients\",\n",
    "        \"food_role\",\n",
    "        \"ingredients\",\n",
    "        \"hasDairy\",\n",
    "        \"hasNuts\",\n",
    "        \"hasMeat\",\n",
    "        \"prep_time\",\n",
    "        \"cook_time\",\n",
    "        \"serves\",\n",
    "        \"instructions\"\n",
    "    ],\n",
    "    \"properties\": {\n",
    "        \"recipe_name\": {\"type\": \"string\"},\n",
    "        \"macronutrients\": {\n",
    "            \"type\": \"object\",\n",
    "            \"patternProperties\": {\n",
    "                \"^.*$\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"measure\": {\"type\": \"string\"},\n",
    "                        \"unit\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"food_role\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"Main Course\", \"Side Dish\", \"Beverage\", \"Dessert\"]\n",
    "            }\n",
    "        },\n",
    "        \"ingredients\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"quantity\": {\"type\": \"object\"},\n",
    "                    \"unit\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"hasDairy\": {\"type\": \"boolean\"},\n",
    "        \"hasNuts\": {\"type\": \"boolean\"},\n",
    "        \"hasMeat\": {\"type\": \"boolean\"},\n",
    "        \"prep_time\": {\"type\": \"string\"},\n",
    "        \"cook_time\": {\"type\": \"string\"},\n",
    "        \"serves\": {\"type\": \"number\"},\n",
    "        \"instructions\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"original_text\": {\"type\": \"string\"},\n",
    "                    \"input_condition\": {\"type\": \"array\"},\n",
    "                    \"tasks\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"action_name\": {\"type\": \"string\"},\n",
    "                                \"output_quality\": {\"type\": \"array\"},\n",
    "                                \"background_knowledge\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"tool\": {\"type\": \"array\"},\n",
    "                                        \"failure\": {\"type\": \"array\"}\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "id": "31b788bba0c8b77d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:31:49.281584Z",
     "start_time": "2025-01-22T16:31:49.276319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a reference response (this would typically be human-annotated data)\n",
    "reference_response = {\n",
    "    \"recipe_name\": \"Peekaboo Sugar Eggs\",\n",
    "    \"macronutrients\": {},  # No nutritional information provided\n",
    "    \"food_role\": [\"Dessert\"],\n",
    "    \"ingredients\": [\n",
    "        {\n",
    "            \"name\": \"granulated sugar\",\n",
    "            \"quantity\": \"4\",\n",
    "            \"unit\": \"cups\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"powdered sugar\",\n",
    "            \"quantity\": \"3/4\",\n",
    "            \"unit\": \"cup\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"egg white\",\n",
    "            \"quantity\": \"1\",\n",
    "            \"unit\": \"whole\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"food coloring\",\n",
    "            \"quantity\": \"as needed\",\n",
    "            \"unit\": \"drops\"\n",
    "        }\n",
    "    ],\n",
    "    \"hasDairy\": False,\n",
    "    \"hasNuts\": False,\n",
    "    \"hasMeat\": False,\n",
    "    \"prep_time\": \"30 minutes\",\n",
    "    \"cook_time\": \"20 minutes\",\n",
    "    \"serves\": 2,  # Makes two 3-inch eggs\n",
    "    \"instructions\": [\n",
    "        {\n",
    "            \"original_text\": \"In a small bowl, mix the egg white with food coloring until the color is evenly distributed and the egg is frothy.\",\n",
    "            \"input_condition\": [\"have_egg_white\", \"have_food_coloring\"],\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"action_name\": \"mix egg white and food coloring\",\n",
    "                    \"output_quality\": [\"color evenly distributed\", \"mixture is frothy\"],\n",
    "                    \"background_knowledge\": {\n",
    "                        \"tool\": [\"small bowl\", \"mixing utensil\"],\n",
    "                        \"failure\": [\"uneven color distribution\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        # ... more instructions would follow\n",
    "    ]\n",
    "}"
   ],
   "id": "c3260e0ddc2f7ca6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:31:50.003151Z",
     "start_time": "2025-01-22T16:31:49.284695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize our metrics\n",
    "bert_score = BERTScore()\n",
    "schema_metric = SchemaAwareMetric(bert_score)\n",
    "prompt_metric = PromptAwareMetric(bert_score)\n",
    "aggregated_metric = AggregatedSimilarityMetric(bert_score)"
   ],
   "id": "1ed99787be0a9b2e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate Metrics",
   "id": "aeaeb94256a05932"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Schema-based evaluation\n",
    "This metric checks if the LLM's output strictly adheres to our predefined JSON schema for recipes. The schema requires specific fields like `recipe_name`, `ingredients`, `instructions`, etc., with defined data types and structures."
   ],
   "id": "8bff4a64cc5b1320"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:31:50.023300Z",
     "start_time": "2025-01-22T16:31:50.004670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "schema_score = schema_metric.calculate_with_prompt(\n",
    "    json.dumps(reference_response),\n",
    "    recipe_data[\"llm_response\"],\n",
    "    recipe_data[\"system_prompt\"],\n",
    "    metadata={\"schema\": RECIPE_SCHEMA}\n",
    ")\n",
    "\n",
    "print(f\"Schema-based score: {schema_score}\")"
   ],
   "id": "d208ca8de6a2e3a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema-based score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitingupta/usc/aiisc/libraries/GenAIResultsComparator/examples/llm_aware_metrics/code/schema_based.py:72: UserWarning: One or more outputs do not match schema. Returning 0.0.\n",
      "  warn(\"One or more outputs do not match schema. Returning 0.0.\")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our score of 0.0 indicates that the LLM's output didn't perfectly match our schema. This is expected because:\n",
    "- Recipe parsing is a complex task requiring understanding of both content and structure\n",
    "- The LLM needs to transform unstructured text into highly structured JSON\n",
    "- Our schema validation is binary (pass/fail) and quite strict\n",
    "- Even small deviations from the expected structure result in a failed validation"
   ],
   "id": "34a3c6586f1143dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompt-aware evaluation\n",
    "This metric evaluates how well the LLM's output captures the content while considering the context provided in the system and user prompts."
   ],
   "id": "9a50c126bbdb3f31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:31:50.926832Z",
     "start_time": "2025-01-22T16:31:50.024842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Prompt-aware evaluation\n",
    "prompt_score = prompt_metric.calculate_with_prompt(\n",
    "    json.dumps(reference_response),\n",
    "    recipe_data[\"llm_response\"],\n",
    "    recipe_data[\"system_prompt\"],\n",
    "    recipe_data[\"user_prompt\"]\n",
    ")\n",
    "\n",
    "print(f\"Prompt-aware score: {prompt_score}\")"
   ],
   "id": "cfaee85856c94d5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt-aware score: {'precision': 0.5313692688941956, 'recall': 0.4975050091743469, 'f1': 0.5138798356056213}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Using BERTScore as the base metric, we get three scores:\n",
    "- Precision: 0.53 - How much of the LLM's output is relevant\n",
    "- Recall: 0.50 - How much of the expected content is captured\n",
    "- F1: 0.51 - Harmonic mean of precision and recall\n",
    "\n",
    "These scores indicate that:\n",
    "- The LLM captured about 50% of the expected content\n",
    "- There's a good balance between precision and recall\n",
    "- The model understood the basic recipe structure but missed some details"
   ],
   "id": "61037104ae0506e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Aggregated similarity score evaluation\n",
    "This metric measures how similar the LLM's output is to the reference response, focusing on key elements like `recipe_name`, `ingredients`, and `instructions`."
   ],
   "id": "7f3e4c5690aeccd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:31:52.052906Z",
     "start_time": "2025-01-22T16:31:50.927925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Alignment score evaluation\n",
    "alignment_score = aggregated_metric.calculate_with_prompt(\n",
    "    json.dumps(reference_response),\n",
    "    recipe_data[\"llm_response\"],\n",
    "    recipe_data[\"system_prompt\"],\n",
    "    recipe_data[\"user_prompt\"],\n",
    "    metadata={\n",
    "        \"key_elements\": [\n",
    "            \"recipe_name\",\n",
    "            \"ingredients\",\n",
    "            \"instructions\",\n",
    "            \"prep_time\",\n",
    "            \"cook_time\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Aggregation score: {alignment_score}\")"
   ],
   "id": "e842bed738e8fe58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment score: {'precision': 0.5740346511205038, 'recall': 0.5878864526748657, 'f1': 0.5779176155726115}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The scores are:\n",
    "- Precision: 0.57 - Higher than prompt-aware, suggesting good adherence to prompt requirements\n",
    "- Recall: 0.59 - Better coverage of expected elements\n",
    "- F1: 0.58 - Overall better performance when considering prompt alignment\n",
    "\n",
    "The aggregated similarity scores are higher because:\n",
    "- This metric considers the relationship between prompt requirements and output\n",
    "- It's more forgiving of structural deviations while focusing on content alignment\n",
    "- The LLM better captured the essential recipe elements requested in the prompt"
   ],
   "id": "84dabd77fd28831a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analysis of Results\n",
    "\n",
    "The combined metrics tell us that:\n",
    "1. While the LLM failed to produce perfectly structured JSON (schema score: 0.0), it did capture meaningful recipe information\n",
    "2. The content quality is moderate (prompt-aware F1: 0.51), suggesting room for improvement in content extraction\n",
    "3. The output shows good alignment with prompt requirements (alignment F1: 0.58)\n",
    "\n",
    "### Areas for Improvement\n",
    "1. JSON Structure: The LLM needs better guidance on producing valid JSON\n",
    "2. Content Extraction: Some recipe details were missed or incorrectly formatted\n",
    "3. Schema Compliance: A more flexible schema validation approach might be beneficial\n",
    "\n",
    "### Next Steps\n",
    "1. Consider using a more lenient schema validation approach\n",
    "2. Add intermediate structure validation to guide the LLM\n",
    "3. Experiment with different prompt formats to improve JSON structure adherence"
   ],
   "id": "fca985ba368981eb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
