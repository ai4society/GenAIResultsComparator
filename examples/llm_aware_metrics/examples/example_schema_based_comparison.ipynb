{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eef923a-b87e-4026-8f68-b138704c060c",
   "metadata": {},
   "source": [
    "# Schema-Based Comparison Example\n",
    "\n",
    "This notebook demonstrates how to use the Schema-Based Metric to compare structured LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06cb4fda-d619-4d3d-9c7a-20fb9e704374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.llm_aware_metrics.code.schema_based import SchemaAwareMetric\n",
    "from llm_metrics.text_similarity_metrics import CosineSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb4bd0-0265-4755-ba62-93b1edd0250a",
   "metadata": {},
   "source": [
    "## Define JSON Schema\n",
    "\n",
    "Let's create a schema for product reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9177a41a-10b4-4f16-aeda-5eb2c55dd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"rating\": {\"type\": \"number\", \"minimum\": 1, \"maximum\": 5},\n",
    "        \"title\": {\"type\": \"string\"},\n",
    "        \"text\": {\"type\": \"string\"},\n",
    "        \"pros\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"cons\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "    },\n",
    "    \"required\": [\"rating\", \"text\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea2126-a69a-4ab0-88be-118b7ad2c01b",
   "metadata": {},
   "source": [
    "## Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1516f4c9-4729-4fcc-85bd-cb11acc5ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid JSON responses\n",
    "response1 = '''\n",
    "{\n",
    "    \"rating\": 4,\n",
    "    \"title\": \"Great product!\",\n",
    "    \"text\": \"This product exceeded my expectations.\",\n",
    "    \"pros\": [\"Easy to use\", \"Good value\"],\n",
    "    \"cons\": [\"Packaging could be better\"]\n",
    "}\n",
    "'''\n",
    "\n",
    "response2 = '''\n",
    "{\n",
    "    \"rating\": 4,\n",
    "    \"text\": \"Really satisfied with this purchase.\",\n",
    "    \"pros\": [\"User friendly\", \"Worth the price\"],\n",
    "    \"cons\": [\"Poor packaging\"]\n",
    "}\n",
    "'''\n",
    "\n",
    "# Invalid JSON response (missing required field)\n",
    "invalid_response = '''\n",
    "{\n",
    "    \"rating\": 4,\n",
    "    \"pros\": [\"Easy to use\"],\n",
    "    \"cons\": [\"Expensive\"]\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12bb81-c0bb-4b9d-8244-dd95399ad3a7",
   "metadata": {},
   "source": [
    "## Using the Schema-Based Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5245353-3f27-4692-bbde-801b10b9247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the metric\n",
    "base_metric = CosineSimilarity()\n",
    "schema_metric = SchemaAwareMetric(base_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f42d1c-356f-4cc3-bd53-6815b066b925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score for valid responses: 0.313\n"
     ]
    }
   ],
   "source": [
    "# Compare valid responses\n",
    "score1 = schema_metric.calculate_with_prompt(\n",
    "    response1,\n",
    "    response2,\n",
    "    \"Write a product review.\",\n",
    "    metadata={\"schema\": review_schema}\n",
    ")\n",
    "\n",
    "print(f\"Similarity score for valid responses: {score1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25281b8a-385c-434f-9bc5-65b27ba291d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score with invalid response: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Compare with invalid response\n",
    "score2 = schema_metric.calculate_with_prompt(\n",
    "    response1,\n",
    "    invalid_response,\n",
    "    \"Write a product review.\",\n",
    "    metadata={\"schema\": review_schema}\n",
    ")\n",
    "\n",
    "print(f\"Similarity score with invalid response: {score2:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
