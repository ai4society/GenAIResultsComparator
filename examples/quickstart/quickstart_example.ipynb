{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fe3ce33ac104fe",
   "metadata": {},
   "source": [
    "# Using **GenAIResultsComparator**: A Simple BLEU Example\n",
    "\n",
    "This notebook demonstrates how to use the GenAIResultsComparator library to evaluate generated text using the BLEU (Bilingual Evaluation Understudy) score. BLEU is one of the most popular metrics for evaluating machine-generated text against reference text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10621881961c17",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's import the necessary modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0cbc954c93e687f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from llm_metrics import BLEU\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f41160a900a49b",
   "metadata": {},
   "source": [
    "## 2. Understanding BLEU Score\n",
    "\n",
    "BLEU score measures how similar the machine-generated text is to reference text(s) by computing the n-gram precision. The score ranges from 0 to 1, where:\n",
    "\n",
    "- 1 indicates a perfect match\n",
    "- 0 indicates no match\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "- BLEU considers word order\n",
    "- It can handle multiple references\n",
    "- It includes a brevity penalty for short translations\n",
    "- It typically uses n-grams up to length 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede306992cd2f12",
   "metadata": {},
   "source": [
    "## 3. Basic Usage\n",
    "\n",
    "Let's start with a simple example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0b6fe9ff290ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.2056\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BLEU metric calculator\n",
    "bleu = BLEU(n=4)  # Using up to 4-grams\n",
    "\n",
    "# Example texts\n",
    "reference = \"The cat sits on the mat.\"\n",
    "generated = \"The cat is sitting on the mat.\"\n",
    "\n",
    "# Calculate BLEU score\n",
    "score = bleu.calculate(generated, reference)\n",
    "\n",
    "print(f\"BLEU Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f1dd0107669a4",
   "metadata": {},
   "source": [
    "## 4. Batch Processing\n",
    "\n",
    "The library also supports batch processing for multiple text pairs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7373bc0ca18dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 BLEU Score: 0.2056\n",
      "Example 2 BLEU Score: 0.0863\n",
      "Example 3 BLEU Score: 0.0707\n",
      "\n",
      "Average BLEU Score: 0.1209\n"
     ]
    }
   ],
   "source": [
    "# Multiple examples\n",
    "references = [\n",
    "    \"The cat sits on the mat.\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"I love reading interesting books.\",\n",
    "]\n",
    "\n",
    "generated_texts = [\n",
    "    \"The cat is sitting on the mat.\",\n",
    "    \"Today the weather is very nice.\",\n",
    "    \"I enjoy reading fascinating books.\",\n",
    "]\n",
    "\n",
    "# Calculate batch scores using `use_corpus_bleu` as False\n",
    "# This parameter is used to specify whether to use corpus-level BLEU or sentence-level BLEU\n",
    "scores = bleu.batch_calculate(generated_texts, references, use_corpus_bleu=False)\n",
    "\n",
    "# Print individual scores\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Example {i + 1} BLEU Score: {score:.4f}\")\n",
    "\n",
    "# Print average score\n",
    "print(f\"\\nAverage BLEU Score: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3d1664d2c9e44",
   "metadata": {},
   "source": [
    "## 5. Advanced Usage: Customizing BLEU Calculation\n",
    "\n",
    "BLEU can be customized in several ways:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c6ca75b5184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default weights BLEU Score: 0.2056\n",
      "Custom weights BLEU Score: 0.3558\n"
     ]
    }
   ],
   "source": [
    "# Using different n-gram weights\n",
    "additional_params = {\n",
    "    \"weights\": (0.4, 0.3, 0.2, 0.1)  # Custom weights for 1-gram to 4-gram\n",
    "}\n",
    "\n",
    "# Compare scores with default and custom weights\n",
    "score_default = bleu.calculate(generated, reference)\n",
    "score_custom = bleu.calculate(generated, reference, additional_params=additional_params)\n",
    "\n",
    "print(f\"Default weights BLEU Score: {score_default:.4f}\")\n",
    "print(f\"Custom weights BLEU Score: {score_custom:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f10e077045f9d0",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "BLEU score is a useful metric for evaluating text similarity, particularly in machine translation tasks. However, it's best used:\n",
    "\n",
    "- In combination with other metrics\n",
    "- With multiple references when possible\n",
    "- For longer texts\n",
    "- As part of a broader evaluation strategy\n",
    "\n",
    "Remember that no single metric is perfect, and human evaluation is often necessary for comprehensive assessment of generated text quality.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
