{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f1e205",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right, #4b6cb7, #182848); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\">\n",
    "    <h1 style=\"color: white; margin: 0; font-size: 2.5em; font-weight: 700;\">GenAIResultsComparator</h1>\n",
    "    <p style=\"color:hsl(0, 0.00%, 87.80%); margin-top: 10px; font-style: italic; font-size: 1.2em; text-align: center;\">A Simple BLEU Example</p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Welcome to the <b>GenAIResultsComparator</b> library! This notebook demonstrates how to use the library to evaluate generated text using the **BLEU** (Bilingual Evaluation Understudy) score.\n",
    "\n",
    "BLEU is one of the most popular metrics for evaluating machine-generated text against reference text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f41160a900a49b",
   "metadata": {},
   "source": [
    "### Understanding BLEU Score\n",
    "\n",
    "BLEU score measures how similar the machine-generated text is to reference text(s) by computing the n-gram precision. The score ranges from 0 to 1, where:\n",
    "\n",
    "- 1 indicates a perfect match\n",
    "- 0 indicates no match\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "- BLEU considers word order\n",
    "- It can handle multiple references\n",
    "- It includes a brevity penalty for short translations\n",
    "- It typically uses n-grams up to length 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10621881961c17",
   "metadata": {},
   "source": [
    "### 1. Installation and Setup\n",
    "\n",
    "For this notebook, we'll assume the library is installed or made accessible via the path modification below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3ba469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Construct the path to the project root (one level up)\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, os.pardir))\n",
    "\n",
    "# Add project root to the system path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20092284",
   "metadata": {},
   "source": [
    "Now, that the `llm_metrics` package is confirmed importable, let's continue by importing necessary modules from the library and other common packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0cbc954c93e687f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from llm_metrics import BLEU\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede306992cd2f12",
   "metadata": {},
   "source": [
    "### 2. Basic Usage\n",
    "\n",
    "Let's start with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0b6fe9ff290ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.2056\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BLEU metric calculator\n",
    "bleu = BLEU(n=4)  # Using up to 4-grams\n",
    "\n",
    "# Example texts\n",
    "reference = \"The cat sits on the mat.\"\n",
    "generated = \"The cat is sitting on the mat.\"\n",
    "\n",
    "# Calculate BLEU score\n",
    "score = bleu.calculate(generated, reference)\n",
    "\n",
    "print(f\"BLEU Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f1dd0107669a4",
   "metadata": {},
   "source": [
    "### 3. Batch Processing\n",
    "\n",
    "The library also supports batch processing for multiple text pairs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7373bc0ca18dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 BLEU Score: 0.2056\n",
      "Example 2 BLEU Score: 0.0863\n",
      "Example 3 BLEU Score: 0.0707\n",
      "\n",
      "Average BLEU Score: 0.1209\n"
     ]
    }
   ],
   "source": [
    "# Multiple examples\n",
    "references = [\n",
    "    \"The cat sits on the mat.\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"I love reading interesting books.\",\n",
    "]\n",
    "\n",
    "generated_texts = [\n",
    "    \"The cat is sitting on the mat.\",\n",
    "    \"Today the weather is very nice.\",\n",
    "    \"I enjoy reading fascinating books.\",\n",
    "]\n",
    "\n",
    "# Calculate batch scores using `use_corpus_bleu` as False\n",
    "# This parameter is used to specify whether to use corpus-level BLEU or sentence-level BLEU\n",
    "scores = bleu.calculate(generated_texts, references, use_corpus_bleu=False)\n",
    "\n",
    "# Print individual scores\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Example {i + 1} BLEU Score: {score:.4f}\")\n",
    "\n",
    "# Print average score\n",
    "print(f\"\\nAverage BLEU Score: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3d1664d2c9e44",
   "metadata": {},
   "source": [
    "### 4. Advanced Usage: Customizing BLEU Calculation\n",
    "\n",
    "BLEU can be customized in several ways:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937c6ca75b5184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default weights BLEU Score: 0.2056\n",
      "Custom weights BLEU Score: 0.3558\n"
     ]
    }
   ],
   "source": [
    "# Using different n-gram weights\n",
    "additional_params = {\n",
    "    \"weights\": (0.4, 0.3, 0.2, 0.1)  # Custom weights for 1-gram to 4-gram\n",
    "}\n",
    "\n",
    "# Compare scores with default and custom weights\n",
    "score_default = bleu.calculate(generated, reference)\n",
    "score_custom = bleu.calculate(generated, reference, **additional_params)\n",
    "\n",
    "print(f\"Default weights BLEU Score: {score_default:.4f}\")\n",
    "print(f\"Custom weights BLEU Score: {score_custom:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f10e077045f9d0",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    "BLEU score is a useful metric for evaluating text similarity, particularly in machine translation tasks. However, it's best used:\n",
    "\n",
    "- In combination with other metrics\n",
    "- With multiple references when possible\n",
    "- For longer texts\n",
    "- As part of a broader evaluation strategy\n",
    "\n",
    "Remember that no single metric is perfect, and human evaluation is often necessary for comprehensive assessment of generated text quality.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
