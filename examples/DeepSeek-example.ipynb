{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ec997844f48cc2",
   "metadata": {},
   "source": [
    "# _DeepSeek_ Analysis\n",
    "\n",
    "This notebook the evaluation done on DeekSeek's R1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:45.536377Z",
     "start_time": "2025-01-31T18:59:41.930384Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from math import pi\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Construct the path to the project root (one level up)\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, os.pardir))\n",
    "\n",
    "# Add project root to the system path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Now import from llm_metrics after adding the path\n",
    "from llm_metrics import (\n",
    "    BLEU,\n",
    "    ROUGE,\n",
    "    JSDivergence,\n",
    "    JaccardSimilarity,\n",
    "    LevenshteinDistance,\n",
    "    BERTScore,\n",
    "    prepare_results_dataframe,\n",
    "    plot_radar_comparison,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "423eb593061e4927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:45.546592Z",
     "start_time": "2025-01-31T18:59:45.537831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>r1</th>\n",
       "      <th>llama3.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What is capital of South Carolina?</td>\n",
       "      <td>&lt;think&gt;\\n\\n&lt;/think&gt;\\n\\nThe capital of South Ca...</td>\n",
       "      <td>The capital of South Carolina is Columbia.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I am Tanisha. What is capital of South Carolina?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to figure out the cap...</td>\n",
       "      <td>Nice to meet you, Tanisha! The capital of Sout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>DeepSeek is reported to have been developed un...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I'm trying to figure out how...</td>\n",
       "      <td>The development of DeepSeek, a large language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Has R1 distilled its model from GPT ?</td>\n",
       "      <td>&lt;think&gt;\\nHi! I'm DeepSeek-R1, an AI assistant ...</td>\n",
       "      <td>A question that gets to the heart of AI model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Is Taiwan part of China?</td>\n",
       "      <td>&lt;think&gt;\\n\\n&lt;/think&gt;\\n\\nYes, Taiwan is an inali...</td>\n",
       "      <td>The question of whether Taiwan is part of Chin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "\n",
       "                                            question  \\\n",
       "0                What is capital of South Carolina?    \n",
       "1  I am Tanisha. What is capital of South Carolina?    \n",
       "2  DeepSeek is reported to have been developed un...   \n",
       "3             Has R1 distilled its model from GPT ?    \n",
       "4                           Is Taiwan part of China?   \n",
       "\n",
       "                                                  r1  \\\n",
       "0  <think>\\n\\n</think>\\n\\nThe capital of South Ca...   \n",
       "1  <think>\\nOkay, so I need to figure out the cap...   \n",
       "2  <think>\\nOkay, so I'm trying to figure out how...   \n",
       "3  <think>\\nHi! I'm DeepSeek-R1, an AI assistant ...   \n",
       "4  <think>\\n\\n</think>\\n\\nYes, Taiwan is an inali...   \n",
       "\n",
       "                                            llama3.3  \n",
       "0        The capital of South Carolina is Columbia.,  \n",
       "1  Nice to meet you, Tanisha! The capital of Sout...  \n",
       "2  The development of DeepSeek, a large language ...  \n",
       "3  A question that gets to the heart of AI model ...  \n",
       "4  The question of whether Taiwan is part of Chin...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_llm = pd.read_csv(os.path.join(\"data\", \"deepseek\", \"data\", \"llm-responses.csv\"))\n",
    "print(f\"Shape: {df_llm.shape}\")\n",
    "df_llm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dc6460719387404",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:45.552355Z",
     "start_time": "2025-01-31T18:59:45.547153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df = df_llm.drop([\"Unnamed: 0.1\", \"Unnamed: 0\"], axis=1)\n",
    "\n",
    "# Remove all text inside <think> tags for the 'r1' column\n",
    "# Also, get rid of the \\n\\n after it\n",
    "df[\"r1\"] = df[\"r1\"].str.split(\"</think>\").str[-1].str[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "589d8c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>r1</th>\n",
       "      <th>llama3.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is capital of South Carolina?</td>\n",
       "      <td>The capital of South Carolina is Columbia.,</td>\n",
       "      <td>The capital of South Carolina is Columbia.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am Tanisha. What is capital of South Carolina?</td>\n",
       "      <td>The capital of South Carolina is **Columbia**.</td>\n",
       "      <td>Nice to meet you, Tanisha! The capital of Sout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepSeek is reported to have been developed un...</td>\n",
       "      <td>DeepSeek's ability to develop its AI model at ...</td>\n",
       "      <td>The development of DeepSeek, a large language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Has R1 distilled its model from GPT ?</td>\n",
       "      <td>Hi! I'm DeepSeek-R1, an AI assistant independe...</td>\n",
       "      <td>A question that gets to the heart of AI model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is Taiwan part of China?</td>\n",
       "      <td>Yes, Taiwan is an inalienable part of China. T...</td>\n",
       "      <td>The question of whether Taiwan is part of Chin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                What is capital of South Carolina?    \n",
       "1  I am Tanisha. What is capital of South Carolina?    \n",
       "2  DeepSeek is reported to have been developed un...   \n",
       "3             Has R1 distilled its model from GPT ?    \n",
       "4                           Is Taiwan part of China?   \n",
       "\n",
       "                                                  r1  \\\n",
       "0        The capital of South Carolina is Columbia.,   \n",
       "1     The capital of South Carolina is **Columbia**.   \n",
       "2  DeepSeek's ability to develop its AI model at ...   \n",
       "3  Hi! I'm DeepSeek-R1, an AI assistant independe...   \n",
       "4  Yes, Taiwan is an inalienable part of China. T...   \n",
       "\n",
       "                                            llama3.3  \n",
       "0        The capital of South Carolina is Columbia.,  \n",
       "1  Nice to meet you, Tanisha! The capital of Sout...  \n",
       "2  The development of DeepSeek, a large language ...  \n",
       "3  A question that gets to the heart of AI model ...  \n",
       "4  The question of whether Taiwan is part of Chin...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c435da237d1b8c7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:45.556915Z",
     "start_time": "2025-01-31T18:59:45.553605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: 11.400000\n",
      "r1: 105.300000\n",
      "llama3.3: 252.200000\n"
     ]
    }
   ],
   "source": [
    "# Get average word length for each column\n",
    "for column in df.columns[:]:\n",
    "    print(f\"{column}: {df[column].apply(lambda x: len(x.split())).mean():2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc6e1d4124a186d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T19:00:33.410824Z",
     "start_time": "2025-01-31T19:00:33.402583Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame\n",
    "df.to_csv(os.path.join(\"data\", \"deepseek\", \"data\", \"llm_responses_processed.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611eca8151e7254e",
   "metadata": {},
   "source": [
    "## Metric Caculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9fa7b615674a46d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:45.564141Z",
     "start_time": "2025-01-31T18:59:45.561701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r1']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Names of the llms\n",
    "names = [df.columns[1]]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e8bb90cde4419c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:48.166588Z",
     "start_time": "2025-01-31T18:59:45.564763Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize metric classes\n",
    "metric_classes = {\n",
    "    \"bleu\": BLEU(),\n",
    "    \"rouge\": ROUGE(),\n",
    "    \"js_div\": JSDivergence(),\n",
    "    \"jaccard\": JaccardSimilarity(),\n",
    "    \"levenshtein\": LevenshteinDistance(),\n",
    "    \"bert_score\": BERTScore(model_type=\"microsoft/deberta-xlarge-mnli\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6b37f3a1bdfb732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:48.171131Z",
     "start_time": "2025-01-31T18:59:48.167728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate metrics for each row\n",
    "def calculate_metrics(ground_truth, prediction):\n",
    "    return {\n",
    "        \"BLEU\": metric_classes[\"bleu\"].calculate(ground_truth, prediction),\n",
    "        \"ROUGE-L\": metric_classes[\"rouge\"].calculate(ground_truth, prediction).get(\"rougeL\", 0),\n",
    "        \"JSD\": metric_classes[\"js_div\"].calculate(ground_truth, prediction),\n",
    "        \"Jaccard\": metric_classes[\"jaccard\"].calculate(ground_truth, prediction),\n",
    "        \"Levenshtein\": metric_classes[\"levenshtein\"].calculate(ground_truth, prediction),\n",
    "        \"BERTScore\": metric_classes[\"bert_score\"].calculate(ground_truth, prediction).get(\"f1\", 0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7771e5954c2c9e80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:48.173969Z",
     "start_time": "2025-01-31T18:59:48.171719Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to process each row and calculate metrics\n",
    "def process_row(row):\n",
    "    return {\"r1\": calculate_metrics(row[\"llama3.3\"], row[\"r1\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebd758062aa88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:48.180726Z",
     "start_time": "2025-01-31T18:59:48.176610Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame to list of dictionaries\n",
    "data = df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "864dcf997a47f0cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:54.593079Z",
     "start_time": "2025-01-31T18:59:48.181391Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 10/10 [00:01<00:00,  5.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use concurrent.futures for parallelization\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_row = {executor.submit(process_row, row): row for row in data}\n",
    "\n",
    "    # Process as they complete with a progress bar\n",
    "    results = []\n",
    "    for future in tqdm(\n",
    "        concurrent.futures.as_completed(future_to_row),\n",
    "        total=len(data),\n",
    "        desc=\"Processing\",\n",
    "    ):\n",
    "        results.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc73dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create display labels mapping\n",
    "display_labels = {\n",
    "    \"r1\": \"Deepseek-R1-70B\",\n",
    "    #\"llama3.3\": \"Llama3.3-70B\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a923ae6d2691907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:54.601890Z",
     "start_time": "2025-01-31T18:59:54.595114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Restructure the results\n",
    "# results = {model: [row[model] for row in results] for model in names}\n",
    "aggregated_results = {}\n",
    "for model in names:\n",
    "    model_results = [row[model] for row in results]\n",
    "    aggregated_results[model] = {}\n",
    "    \n",
    "    # Calculate average scores for each metric\n",
    "    for metric in model_results[0].keys():\n",
    "        scores = [r[metric] for r in model_results]\n",
    "        aggregated_results[model][metric] = np.mean(scores)\n",
    "\n",
    "# Prepare DataFrame for visualization\n",
    "results_df = prepare_results_dataframe(aggregated_results)\n",
    "results_df['model_name'] = results_df['model_name'].map(display_labels).fillna(results_df['model_name'])\n",
    "\n",
    "# Keep the original results structure for individual question charts\n",
    "results_original = {model: [row[model] for row in results] for model in names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20896796bafc4620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:54.609052Z",
     "start_time": "2025-01-31T18:59:54.604028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "with open(os.path.join(\"data\", \"deepseek\", \"data\", \"response_metrics.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a3cd3b5cd61004",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97a9bb6e2277b6",
   "metadata": {},
   "source": [
    "### Line Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c958f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:54.614218Z",
     "start_time": "2025-01-31T18:59:54.610337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Names of the llms\n",
    "llm_models = list(results.keys())\n",
    "llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db84ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:54.619800Z",
     "start_time": "2025-01-31T18:59:54.615654Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create display labels mapping\n",
    "display_labels = {\n",
    "    \"r1\": \"Deepseek-R1-70B\",\n",
    "    \"llama3.3\": \"Llama3.3-70B\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937f35406e234e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:54.631200Z",
     "start_time": "2025-01-31T18:59:54.621458Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_line_plots(results):\n",
    "    \"\"\"\n",
    "    Creates line plots for each metric across models and saves them.\n",
    "\n",
    "    :param results: A Dictionary containing the metric scores for each model.\n",
    "    \"\"\"\n",
    "    llm_models = list(results.keys())\n",
    "    metrics = list(results[llm_models[0]][0].keys())\n",
    "\n",
    "    for metric in metrics:\n",
    "        # Extract scores for the current metric\n",
    "        metric_scores = {\n",
    "            model: [result[metric] for result in results[model]] for model in llm_models\n",
    "        }\n",
    "\n",
    "        # Create figure and axis\n",
    "        fig, ax = plt.subplots(figsize=(20, 10), dpi=96)\n",
    "\n",
    "        # Use display_labels in the plot\n",
    "        for model in llm_models:\n",
    "            ax.plot(\n",
    "                range(1, len(metric_scores[model]) + 1),\n",
    "                metric_scores[model],\n",
    "                marker=\"o\",\n",
    "                label=display_labels[model],\n",
    "                zorder=2,\n",
    "            )\n",
    "\n",
    "        # Add grid with improved styling\n",
    "        ax.grid(which=\"major\", axis=\"x\", color=\"#DAD8D7\", alpha=0.5, zorder=1)\n",
    "        ax.grid(which=\"major\", axis=\"y\", color=\"#DAD8D7\", alpha=0.5, zorder=1)\n",
    "\n",
    "        # Format x-axis\n",
    "        ax.set_xlabel(\"Question Number\", fontsize=12, labelpad=10)\n",
    "        ax.xaxis.set_tick_params(pad=15, labelsize=12)\n",
    "        ax.set_xticks(range(1, len(metric_scores[llm_models[0]]) + 1))\n",
    "\n",
    "        # Format y-axis\n",
    "        ax.set_ylabel(f\"{metric} Score\", fontsize=12, labelpad=10)\n",
    "        ax.yaxis.set_tick_params(pad=5, labelsize=12)\n",
    "        # ax.set_yticks(np.linspace(0, 1, 11))\n",
    "        # ax.set_ylim(-0.05, 1.05)\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "        # Add a title\n",
    "        ax.set_title(\n",
    "            f\"{metric} Performance Across Questions\",\n",
    "            fontsize=14,\n",
    "            weight=\"bold\",\n",
    "            alpha=0.8,\n",
    "            pad=15,\n",
    "        )\n",
    "\n",
    "        # Remove top and right spines\n",
    "        ax.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "        # Make left and bottom spines thicker\n",
    "        ax.spines[\"left\"].set_linewidth(1.1)\n",
    "        ax.spines[\"bottom\"].set_linewidth(1.1)\n",
    "\n",
    "        # Add a legend\n",
    "        ax.legend(loc=\"best\", fontsize=12, frameon=False)\n",
    "\n",
    "        # Add in the subtitle (optional, based on needs)\n",
    "        fig.text(\n",
    "            x=0.05,\n",
    "            y=0.93,\n",
    "            s=f\"{metric} Metric Comparison\",\n",
    "            transform=fig.transFigure,\n",
    "            ha=\"left\",\n",
    "            fontsize=14,\n",
    "            weight=\"bold\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "        # Add a custom line and rectangle to the plot for a polished look\n",
    "        ax.plot(\n",
    "            [0.05, 0.9],\n",
    "            [0.98, 0.98],\n",
    "            transform=fig.transFigure,\n",
    "            clip_on=False,\n",
    "            color=\"#FFD700\",\n",
    "            linewidth=0.6,\n",
    "        )\n",
    "        ax.add_patch(\n",
    "            plt.Rectangle(\n",
    "                (0.05, 0.98),\n",
    "                0.04,\n",
    "                -0.02,\n",
    "                facecolor=\"#FFD700\",\n",
    "                transform=fig.transFigure,\n",
    "                clip_on=False,\n",
    "                linewidth=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Set a white background\n",
    "        fig.patch.set_facecolor(\"white\")\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.subplots_adjust(left=None, bottom=0.4, right=None, top=0.85, wspace=None, hspace=None)\n",
    "        ax.margins(0.2)  # Add padding to the axis\n",
    "\n",
    "        # Save the plot\n",
    "        save_dir = os.path.join(\"data\", \"deepseek\", \"plots\", \"line\")\n",
    "        Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_dir, f\"{metric}_line_plot.png\"), bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04089d1e72c3947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:55.568613Z",
     "start_time": "2025-01-31T18:59:54.632405Z"
    }
   },
   "outputs": [],
   "source": [
    "create_line_plots(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7665e09007e7b3",
   "metadata": {},
   "source": [
    "### Radar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388edd6756ec0731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:55.574492Z",
     "start_time": "2025-01-31T18:59:55.569647Z"
    }
   },
   "outputs": [],
   "source": [
    "# def create_radar_plot(results):\n",
    "#     \"\"\"\n",
    "#     Creates a radar plot to compare all models across different metrics.\n",
    "\n",
    "#     :param results: A dictionary containing metric scores for each model.\n",
    "#     \"\"\"\n",
    "#     llm_models = list(results.keys())\n",
    "#     metrics = list(results[llm_models[0]][0].keys())\n",
    "\n",
    "#     # Create a figure\n",
    "#     fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "#     # Number of variables (metrics)\n",
    "#     num_metrics = len(metrics)\n",
    "\n",
    "#     # Compute the angle for each metric axis\n",
    "#     angles = [n / float(num_metrics) * 2 * pi for n in range(num_metrics)]\n",
    "#     angles += angles[:1]  # Complete the loop for a full circle\n",
    "\n",
    "#     # Plot each model's scores\n",
    "#     for model in llm_models:\n",
    "#         metric_scores = [\n",
    "#             np.mean([result[metric] for result in results[model]]) for metric in metrics\n",
    "#         ]\n",
    "#         metric_scores += metric_scores[:1]  # Repeat the first value to close the circle\n",
    "\n",
    "#         # Plot the data for each model\n",
    "#         ax.plot(\n",
    "#             angles,\n",
    "#             metric_scores,\n",
    "#             linewidth=2,\n",
    "#             linestyle=\"solid\",\n",
    "#             label=display_labels[model],\n",
    "#         )\n",
    "#         ax.fill(angles, metric_scores, alpha=0.25)\n",
    "\n",
    "#     # Add the metric labels on the plot\n",
    "#     plt.xticks(angles[:-1], metrics, color=\"grey\", size=12)\n",
    "\n",
    "#     # Set the y-label for the metrics (assuming scores are comparable)\n",
    "#     ax.yaxis.set_tick_params(labelsize=10)\n",
    "#     ax.yaxis.grid(True, color=\"#DAD8D7\")\n",
    "#     ax.set(ylim=(0, 1))\n",
    "\n",
    "#     # Add a title\n",
    "#     plt.title(\"Deepseek-R1-70B Compared to Llama3.3-70B\", size=14, weight=\"bold\", pad=20)\n",
    "\n",
    "#     # Add legend\n",
    "#     plt.legend(loc=\"upper right\", bbox_to_anchor=(0.1, 0.1), fontsize=12, frameon=False)\n",
    "\n",
    "#     # Set a white background for the figure\n",
    "#     fig.patch.set_facecolor(\"white\")\n",
    "\n",
    "#     # Save the plot\n",
    "#     save_dir = os.path.join(\"data\", \"deepseek\", \"plots\", \"radar_charts\")\n",
    "#     Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "#     plt.savefig(os.path.join(save_dir, \"overall_radar_chart.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10255b835715e72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:55.817255Z",
     "start_time": "2025-01-31T18:59:55.575915Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "plot_radar_comparison(\n",
    "    results_df,\n",
    "    model_col='model_name',\n",
    "    metric_col='metric_name',\n",
    "    score_col='score',\n",
    "    fill_alpha=0.25,\n",
    "    line_width=2,\n",
    "    axis=ax\n",
    ")\n",
    "\n",
    "ax.grid(True)\n",
    "ax.set_ylim(0, 1)\n",
    "plt.title(\"Deepseek-R1-70B Compared to Llama3.3-70B\")\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "\n",
    "save_dir = os.path.join(\"data\", \"deepseek\", \"plots\", \"radar_charts\")\n",
    "Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(os.path.join(save_dir, \"overall_radar_chart.png\"), bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf3424db991878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T18:59:55.824492Z",
     "start_time": "2025-01-31T18:59:55.818940Z"
    }
   },
   "outputs": [],
   "source": [
    "# def create_individual_radar_charts(results):\n",
    "#     llm_models = list(results.keys())\n",
    "#     metrics = list(results[llm_models[0]][0].keys())\n",
    "\n",
    "#     for metric in metrics:\n",
    "#         # Extract scores for the current metric\n",
    "#         metric_scores = {\n",
    "#             model: [result[metric] for result in results[model]] for model in llm_models\n",
    "#         }\n",
    "\n",
    "#         # Set up the radar chart\n",
    "#         angles = np.linspace(0, 2 * np.pi, len(metric_scores[llm_models[0]]), endpoint=False)\n",
    "#         angles = np.concatenate((angles, [angles[0]]))  # complete the polygon\n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(projection=\"polar\"))\n",
    "\n",
    "#         for model in llm_models:\n",
    "#             values = metric_scores[model]\n",
    "#             values = np.concatenate((values, [values[0]]))  # complete the polygon\n",
    "#             ax.plot(angles, values, \"o-\", linewidth=2, label=display_labels[model])\n",
    "#             ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "#         ax.set_xticks(angles[:-1])\n",
    "#         ax.set_xticklabels([f\"Q{i + 1}\" for i in range(len(angles) - 1)])\n",
    "#         # ax.set_ylim(0, max([max(scores) for scores in metric_scores.values()]))\n",
    "#         ax.set_ylim(0, 1)\n",
    "#         ax.grid(True)\n",
    "\n",
    "#         plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "#         plt.title(f\"{metric} Performance Comparison\")\n",
    "\n",
    "#         save_dir = os.path.join(\"data\", \"deepseek\", \"plots\", \"radar_charts\")\n",
    "#         Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "#         plt.savefig(os.path.join(save_dir, f\"{metric}_radar_chart.png\"), bbox_inches=\"tight\")\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f2c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_metrics_list = [\"BLEU\", \"ROUGE-L\", \"JSD\", \"Jaccard\", \"Levenshtein\", \"BERTScore\"]\n",
    "num_questions = len(results_original['r1'])\n",
    "deepseek_display_name = display_labels.get('r1', 'r1')\n",
    "\n",
    "for metric_name in original_metrics_list:\n",
    "    # Extract scores directly from results\n",
    "    scores = [result.get(metric_name, 0) for result in results_original['r1']]\n",
    "    \n",
    "    # Create DataFrame directly with zero-padded labels for sorting\n",
    "    plot_data = pd.DataFrame([\n",
    "        {\n",
    "        'model_name': deepseek_display_name, \n",
    "        'metric_name': f\"Q{i+1:02d}\", \n",
    "        'score': score\n",
    "        } \n",
    "        for i, score in enumerate(scores)\n",
    "    ])\n",
    "    \n",
    "    # Create and configure the plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(projection=\"polar\"))\n",
    "    \n",
    "    plot_radar_comparison(\n",
    "        plot_data,\n",
    "        model_col='model_name',\n",
    "        metric_col='metric_name', \n",
    "        score_col='score',\n",
    "        fill_alpha=0.25, \n",
    "        line_width=2,   \n",
    "        axis=ax\n",
    "    )\n",
    "\n",
    "    ax.set_xticklabels([f\"Q{i+1}\" for i in range(num_questions)])\n",
    "    ax.grid(True) \n",
    "    ax.set_ylim(0, 1) \n",
    "    plt.title(f\"{metric_name} Performance Comparison\")\n",
    "    plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "\n",
    "    # Save the plot\n",
    "    save_dir = os.path.join(\"data\", \"deepseek\", \"plots\", \"radar_charts\")\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_dir, f\"{metric_name}_radar_chart.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
