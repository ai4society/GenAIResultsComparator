{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e923e5c2",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right, #4b6cb7, #182848); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\">\n",
    "    <h1 style=\"color: white; margin: 0; font-size: 2.5em; font-weight: 700;\">GenAIResultsComparator</h1>\n",
    "    <p style=\"color:hsl(0, 0.00%, 87.80%); margin-top: 10px; font-style: italic; font-size: 1.2em; text-align: center;\">A Hands-On on Evaluation w/ Thresholds</p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Welcome to the <b>GenAIResultsComparator</b> library! This notebook will guide you through its features and demonstrate how to use it for evaluating text generated by Large Language Models (LLMs) against reference texts. We'll cover everything from basic setup to calculating various metrics and applying thresholds to the results.\n",
    "\n",
    "This example is designed for technical users who want to:\n",
    "\n",
    "- Compare generated text strings with ground truth versions.\n",
    "- Utilize a range of reference-based evaluation metrics.\n",
    "- Process single pairs or batches of text efficiently.\n",
    "- Applying default & custom thresholds to the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0df06",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252768d6",
   "metadata": {},
   "source": [
    "For this notebook, we'll assume the library is installed or made accessible via the path modification below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b642cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Construct the path to the project root (one level up)\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, os.pardir))\n",
    "\n",
    "# Add project root to the system path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00626c",
   "metadata": {},
   "source": [
    "Now, that the `llm_metrics` package is confirmed importable, let's continue by importing necessary modules from the library and other common packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac548f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from llm_metrics.ngram_metrics import BLEU, ROUGE, JSDivergence\n",
    "from llm_metrics.text_similarity_metrics import (\n",
    "    JaccardSimilarity,\n",
    "    CosineSimilarity,\n",
    "    LevenshteinDistance,\n",
    "    SequenceMatcherSimilarity,\n",
    ")\n",
    "from llm_metrics.semantic_similarity_metrics import BERTScore\n",
    "\n",
    "from llm_metrics.utils import generate_deltas_csv\n",
    "\n",
    "from llm_metrics.thresholds import (\n",
    "    apply_thresholds,\n",
    "    get_default_thresholds,\n",
    "    calculate_pass_fail_percent,\n",
    ")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f6bec",
   "metadata": {},
   "source": [
    "### 2. Initialize Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2ce43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics initialized:\n",
      "  - BLEU\n",
      "  - ROUGE\n",
      "  - JSD\n",
      "  - BERTScore\n",
      "  - Jaccard\n",
      "  - Cosine\n",
      "  - Levenshtein\n",
      "  - SequenceMatcher\n"
     ]
    }
   ],
   "source": [
    "# Initialize all metric instances\n",
    "metrics = {\n",
    "    \"BLEU\": BLEU(),\n",
    "    \"ROUGE\": ROUGE(rouge_types=[\"rougeL\"]),  # Using ROUGE-L for simplicity\n",
    "    \"JSD\": JSDivergence(),\n",
    "    \"BERTScore\": BERTScore(model_type=\"bert-base-uncased\", output_val=[\"f1\"]),\n",
    "    \"Jaccard\": JaccardSimilarity(),\n",
    "    \"Cosine\": CosineSimilarity(),\n",
    "    \"Levenshtein\": LevenshteinDistance(),\n",
    "    \"SequenceMatcher\": SequenceMatcherSimilarity(),\n",
    "}\n",
    "\n",
    "print(\"Metrics initialized:\")\n",
    "for name in metrics.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57153b41",
   "metadata": {},
   "source": [
    "### 3. Sample Data for the implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1bc2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single pair example\n",
    "single_generated = \"The quick brown fox jumps over the lazy dog\"\n",
    "single_reference = \"A quick brown fox jumped over a lazy dog\"\n",
    "\n",
    "# Batch examples - varying quality of translations/paraphrases\n",
    "generated_texts = [\n",
    "    # High quality paraphrases\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A beautiful sunset painted the sky with vibrant colors\",\n",
    "    \"Machine learning models require large amounts of data\",\n",
    "    # Medium quality paraphrases\n",
    "    \"The cat sat on the comfortable mat\",\n",
    "    \"Artificial intelligence is transforming many industries\",\n",
    "    # Lower quality / more different\n",
    "    \"Dogs are loyal companions\",\n",
    "    \"Programming requires logical thinking\",\n",
    "    \"The weather is nice today\",\n",
    "]\n",
    "\n",
    "reference_texts = [\n",
    "    # Corresponding references\n",
    "    \"A quick brown fox jumped over a lazy dog\",\n",
    "    \"The sunset painted beautiful colors across the sky\",\n",
    "    \"Machine learning algorithms need substantial data for training\",\n",
    "    \"The cat was sitting on the mat\",\n",
    "    \"AI technology is revolutionizing various sectors\",\n",
    "    \"Cats make independent pets\",\n",
    "    \"Coding needs systematic problem-solving skills\",\n",
    "    \"It's a pleasant day outside\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106eed69",
   "metadata": {},
   "source": [
    "### 4. Test Case-1: Single Pair Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520bc0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SINGLE PAIR EVALUATION ===\n",
      "\n",
      "BLEU: 0.1562\n",
      "ROUGE: 0.7778\n",
      "JSD: 0.5193\n",
      "BERTScore: 0.9103\n",
      "Jaccard: 0.6000\n",
      "Cosine: 0.6838\n",
      "Levenshtein: 0.8675\n",
      "SequenceMatcher: 0.8675\n",
      "\n",
      "--- Applying Default Thresholds ---\n",
      "Default thresholds: {'BLEU': 0.5, 'ROUGE': 0.5, 'JSD': 0.5, 'BERTScore': 0.5, 'Jaccard': 0.5, 'Cosine': 0.5, 'Levenshtein': 0.5, 'SequenceMatcher': 0.5}\n",
      "\n",
      "--- Threshold Results ---\n",
      "\n",
      "BLEU:\n",
      "  Score: 0.1562\n",
      "  Threshold: 0.5\n",
      "  Passed: False\n",
      "\n",
      "ROUGE:\n",
      "  Score: 0.7778\n",
      "  Threshold: 0.5\n",
      "  Passed: True\n",
      "\n",
      "JSD:\n",
      "  Score: 0.5193\n",
      "  Threshold: 0.5\n",
      "  Passed: False\n",
      "\n",
      "BERTScore:\n",
      "  Score: 0.9103\n",
      "  Threshold: 0.5\n",
      "  Passed: True\n",
      "\n",
      "Jaccard:\n",
      "  Score: 0.6000\n",
      "  Threshold: 0.5\n",
      "  Passed: True\n",
      "\n",
      "Cosine:\n",
      "  Score: 0.6838\n",
      "  Threshold: 0.5\n",
      "  Passed: True\n",
      "\n",
      "Levenshtein:\n",
      "  Score: 0.8675\n",
      "  Threshold: 0.5\n",
      "  Passed: True\n",
      "\n",
      "SequenceMatcher:\n",
      "  Score: 0.8675\n",
      "  Threshold: 0.5\n",
      "  Passed: True\n"
     ]
    }
   ],
   "source": [
    "print(\"=== SINGLE PAIR EVALUATION ===\\n\")\n",
    "\n",
    "# Calculate scores for single pair\n",
    "single_scores = {}\n",
    "for name, metric in metrics.items():\n",
    "    score = metric.calculate(single_generated, single_reference)\n",
    "    single_scores[name] = score\n",
    "    print(f\"{name}: {score:.4f}\" if isinstance(score, (int, float)) else f\"{name}: {score}\")\n",
    "\n",
    "# Apply thresholds\n",
    "print(\"\\n--- Applying Default Thresholds ---\")\n",
    "default_thresholds = get_default_thresholds()\n",
    "print(\"Default thresholds:\", default_thresholds)\n",
    "\n",
    "single_results = apply_thresholds(single_scores)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n--- Threshold Results ---\")\n",
    "for metric_name, details in single_results.items():\n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    print(\n",
    "        f\"  Score: {details['score']:.4f}\"\n",
    "        if isinstance(details[\"score\"], (int, float))\n",
    "        else f\"  Score: {details['score']}\"\n",
    "    )\n",
    "    print(f\"  Threshold: {details['threshold_applied']}\")\n",
    "    print(f\"  Passed: {details['passed_threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24487e22",
   "metadata": {},
   "source": [
    "### 5. Test Case-2: Batch Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb67553e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BATCH EVALUATION ===\n",
      "\n",
      "Calculated scores for 8 text pairs\n",
      "\n",
      "--- Sample Results (First 3 items) ---\n",
      "\n",
      "Item 0:\n",
      "Generated: 'The quick brown fox jumps over the lazy dog...'\n",
      "Reference: 'A quick brown fox jumped over a lazy dog...'\n",
      "  BLEU: 0.156 ✗\n",
      "  ROUGE: 0.778 ✓\n",
      "  JSD: 0.519 ✗\n",
      "  BERTScore: 0.910 ✓\n",
      "  Jaccard: 0.600 ✓\n",
      "  Cosine: 0.684 ✓\n",
      "  Levenshtein: 0.867 ✓\n",
      "  SequenceMatcher: 0.867 ✓\n",
      "\n",
      "Item 1:\n",
      "Generated: 'A beautiful sunset painted the sky with vibrant co...'\n",
      "Reference: 'The sunset painted beautiful colors across the sky...'\n",
      "  BLEU: 0.079 ✗\n",
      "  ROUGE: 0.471 ✗\n",
      "  JSD: 0.549 ✗\n",
      "  BERTScore: 0.790 ✓\n",
      "  Jaccard: 0.600 ✓\n",
      "  Cosine: 0.783 ✓\n",
      "  Levenshtein: 0.519 ✓\n",
      "  SequenceMatcher: 0.462 ✗\n",
      "\n",
      "Item 2:\n",
      "Generated: 'Machine learning models require large amounts of d...'\n",
      "Reference: 'Machine learning algorithms need substantial data ...'\n",
      "  BLEU: 0.065 ✗\n",
      "  ROUGE: 0.375 ✗\n",
      "  JSD: 0.342 ✓\n",
      "  BERTScore: 0.765 ✓\n",
      "  Jaccard: 0.231 ✗\n",
      "  Cosine: 0.375 ✗\n",
      "  Levenshtein: 0.557 ✓\n",
      "  SequenceMatcher: 0.522 ✓\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== BATCH EVALUATION ===\\n\")\n",
    "\n",
    "# Calculate scores for batch\n",
    "batch_scores = []\n",
    "for i in range(len(generated_texts)):\n",
    "    item_scores = {}\n",
    "    for name, metric in metrics.items():\n",
    "        item_scores[name] = metric.calculate(generated_texts[i], reference_texts[i])\n",
    "    batch_scores.append(item_scores)\n",
    "\n",
    "print(f\"Calculated scores for {len(batch_scores)} text pairs\")\n",
    "\n",
    "# Apply thresholds to batch\n",
    "batch_results = apply_thresholds(batch_scores)\n",
    "\n",
    "# Display first few results\n",
    "print(\"\\n--- Sample Results (First 3 items) ---\")\n",
    "for i in range(min(3, len(batch_results))):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(f\"Generated: '{generated_texts[i][:50]}...'\")\n",
    "    print(f\"Reference: '{reference_texts[i][:50]}...'\")\n",
    "    for metric_name, details in batch_results[i].items():\n",
    "        score_str = (\n",
    "            f\"{details['score']:.3f}\"\n",
    "            if isinstance(details[\"score\"], (int, float))\n",
    "            else str(details[\"score\"])\n",
    "        )\n",
    "        passed = \"✓\" if details[\"passed_threshold\"] else \"✗\"\n",
    "        print(f\"  {metric_name}: {score_str} {passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399470a3",
   "metadata": {},
   "source": [
    "### 6. Calculating Pass/Fail Percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8d06f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PASS/FAIL STATISTICS ===\n",
      "\n",
      "Metric Performance Summary:\n",
      "------------------------------------------------------------\n",
      "Metric          Pass Rate    Passed     Failed    \n",
      "------------------------------------------------------------\n",
      "BLEU               0.0%          0          8\n",
      "ROUGE             25.0%          2          6\n",
      "JSD               62.5%          5          3\n",
      "BERTScore        100.0%          8          0\n",
      "Jaccard           37.5%          3          5\n",
      "Cosine            37.5%          3          5\n",
      "Levenshtein       50.0%          4          4\n",
      "SequenceMatcher   37.5%          3          5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== PASS/FAIL STATISTICS ===\\n\")\n",
    "\n",
    "# Extract scores in the format expected by calculate_pass_fail_percentages\n",
    "scores_dict = {}\n",
    "for metric_name in metrics.keys():\n",
    "    scores_dict[metric_name] = [item[metric_name] for item in batch_scores]\n",
    "\n",
    "# Calculate percentages\n",
    "percentages = calculate_pass_fail_percent(scores_dict)\n",
    "\n",
    "# Display results\n",
    "print(\"Metric Performance Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<15} {'Pass Rate':<12} {'Passed':<10} {'Failed':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric, stats in percentages.items():\n",
    "    print(\n",
    "        f\"{metric:<15} {stats['pass_percentage']:>6.1f}% {stats['total_passed']:>10} {stats['total_failed']:>10}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae0f63",
   "metadata": {},
   "source": [
    "### 7. Generating CSV report for the deltas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae0d7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV report generated at: data/threshold/single_pair_analysis.csv\n",
      "CSV report generated at: data/threshold/batch_analysis.csv\n",
      "\n",
      "Sample of batch analysis CSV:\n",
      "                                      generated_text  \\\n",
      "0        The quick brown fox jumps over the lazy dog   \n",
      "1  A beautiful sunset painted the sky with vibran...   \n",
      "2  Machine learning models require large amounts ...   \n",
      "\n",
      "                                      reference_text  BLEU_score  BLEU_passed  \\\n",
      "0           A quick brown fox jumped over a lazy dog    0.156197        False   \n",
      "1  The sunset painted beautiful colors across the...    0.079369        False   \n",
      "2  Machine learning algorithms need substantial d...    0.065006        False   \n",
      "\n",
      "   ROUGE_score  ROUGE_passed  JSD_score  JSD_passed  BERTScore_score  \\\n",
      "0     0.777778          True   0.519324       False         0.910302   \n",
      "1     0.470588         False   0.549008       False         0.790194   \n",
      "2     0.375000         False   0.341808        True         0.765008   \n",
      "\n",
      "   BERTScore_passed  Jaccard_score  Jaccard_passed  Cosine_score  \\\n",
      "0              True       0.600000            True      0.683763   \n",
      "1              True       0.600000            True      0.782624   \n",
      "2              True       0.230769           False      0.375000   \n",
      "\n",
      "   Cosine_passed  Levenshtein_score  Levenshtein_passed  \\\n",
      "0           True           0.867470                True   \n",
      "1           True           0.519231                True   \n",
      "2          False           0.556522                True   \n",
      "\n",
      "   SequenceMatcher_score  SequenceMatcher_passed  \n",
      "0               0.867470                    True  \n",
      "1               0.461538                   False  \n",
      "2               0.521739                    True  \n"
     ]
    }
   ],
   "source": [
    "# Single pair CSV\n",
    "generate_deltas_csv(\n",
    "    single_results,\n",
    "    generated_texts=single_generated,\n",
    "    reference_texts=single_reference,\n",
    "    output_csv_path=os.path.join(\"data\", \"threshold\", \"single_pair_analysis.csv\"),\n",
    ")\n",
    "\n",
    "# Batch CSV\n",
    "generate_deltas_csv(\n",
    "    batch_results,\n",
    "    generated_texts=generated_texts,\n",
    "    reference_texts=reference_texts,\n",
    "    output_csv_path=os.path.join(\"data\", \"threshold\", \"batch_analysis.csv\"),\n",
    ")\n",
    "\n",
    "# Display a sample of the batch CSV\n",
    "print(\"\\nSample of batch analysis CSV:\")\n",
    "df = pd.read_csv(os.path.join(\"data\", \"threshold\", \"batch_analysis.csv\"))\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5d8f5",
   "metadata": {},
   "source": [
    "### 8. Testing with custom Thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d93f5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom (stricter) thresholds:\n",
      "  BLEU: 0.7\n",
      "  ROUGE: 0.65\n",
      "  JSD: 0.3\n",
      "  BERTScore: 0.85\n",
      "  Jaccard: 0.7\n",
      "  Cosine: 0.8\n",
      "  Levenshtein: 0.7\n",
      "  SequenceMatcher: 0.75\n",
      "\n",
      "--- Comparison: Default vs Custom Thresholds ---\n",
      "Metric          Default Pass %  Custom Pass %   Difference\n",
      "-------------------------------------------------------\n",
      "BLEU                      0.0%           0.0%      +0.0%\n",
      "ROUGE                    25.0%          25.0%      +0.0%\n",
      "JSD                      62.5%         100.0%     +37.5%\n",
      "BERTScore               100.0%          25.0%     -75.0%\n",
      "Jaccard                  37.5%           0.0%     -37.5%\n",
      "Cosine                   37.5%           0.0%     -37.5%\n",
      "Levenshtein              50.0%          12.5%     -37.5%\n",
      "SequenceMatcher          37.5%          12.5%     -25.0%\n",
      "CSV report generated at: data/threshold/custom_threshold_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# Defining custom thresholds\n",
    "custom_thresholds = {\n",
    "    \"BLEU\": 0.7,\n",
    "    \"ROUGE\": 0.65,\n",
    "    \"JSD\": 0.3,  # Lower is better for JSD\n",
    "    \"BERTScore\": 0.85,\n",
    "    \"Jaccard\": 0.7,\n",
    "    \"Cosine\": 0.8,\n",
    "    \"Levenshtein\": 0.7,\n",
    "    \"SequenceMatcher\": 0.75,\n",
    "}\n",
    "\n",
    "print(\"Custom (stricter) thresholds:\")\n",
    "for metric, threshold in custom_thresholds.items():\n",
    "    print(f\"  {metric}: {threshold}\")\n",
    "\n",
    "# Apply custom thresholds\n",
    "custom_results = apply_thresholds(batch_scores, thresholds=custom_thresholds)\n",
    "\n",
    "# Calculate new percentages\n",
    "custom_percentages = calculate_pass_fail_percent(scores_dict, thresholds=custom_thresholds)\n",
    "\n",
    "print(\"\\n--- Comparison: Default vs Custom Thresholds ---\")\n",
    "print(f\"{'Metric':<15} {'Default Pass %':<15} {'Custom Pass %':<15} {'Difference':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for metric in metrics.keys():\n",
    "    default_pass = percentages[metric][\"pass_percentage\"]\n",
    "    custom_pass = custom_percentages[metric][\"pass_percentage\"]\n",
    "    diff = custom_pass - default_pass\n",
    "    print(f\"{metric:<15} {default_pass:>13.1f}% {custom_pass:>13.1f}% {diff:>+9.1f}%\")\n",
    "\n",
    "# Generate CSV with custom thresholds\n",
    "generate_deltas_csv(\n",
    "    custom_results,\n",
    "    generated_texts=generated_texts,\n",
    "    reference_texts=reference_texts,\n",
    "    output_csv_path=os.path.join(\"data\", \"threshold\", \"custom_threshold_analysis.csv\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aad297",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook has demonstrated the core functionalities of the `GenAIResultsComparator` library. You've learned how to:\n",
    "\n",
    "- Initialize and use various N-gram, text similarity, and semantic similarity metrics.\n",
    "- Calculate scores for single text pairs and batches of texts.\n",
    "- Evaluate the results on various default and custom threshold settings.\n",
    "\n",
    "The library is designed to be extensible, so you can also create your own custom metrics by inheriting from `BaseMetric`. For more advanced use cases, such as prompt-aware evaluation, check the `examples` folder in the library's repository.\n",
    "\n",
    "We encourage you to explore the library further with your own datasets and LLM outputs. Happy evaluating!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
