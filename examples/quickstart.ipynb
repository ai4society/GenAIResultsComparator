{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1f50a1",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right, #4b6cb7, #182848); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\">\n",
    "    <h1 style=\"color: white; margin: 0; font-size: 2.5em; font-weight: 700;\">GenAIResultsComparator</h1>\n",
    "    <p style=\"color: #e0e0e0; margin-top: 10px; font-style: italic; font-size: 1.2em; text-align: center;\">Quickstart Tutorial</p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Welcome to the <b>GenAIResultsComparator</b> library! This notebook will guide you through its features and demonstrate how to use it for evaluating text generated by Large Language Models (LLMs) against reference texts.\n",
    "\n",
    "This example is designed for technical users who want to:\n",
    "\n",
    "- Compare generated text strings with ground truth versions.\n",
    "- Utilize a range of reference-based evaluation metrics.\n",
    "- Process single pairs or batches of text efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc8af1",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b15c9",
   "metadata": {},
   "source": [
    "For this notebook, we'll assume the library is installed or made accessible via the path modification below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba8346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Construct the path to the project root (one level up)\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, os.pardir))\n",
    "\n",
    "# Add project root to the system path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa2f3e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT score:\n",
      "[{'rouge1': 0.23529411764705882,\n",
      "  'rouge2': 0.13333333333333333,\n",
      "  'rougeL': 0.23529411764705882}]\n"
     ]
    }
   ],
   "source": [
    "from llm_metrics.ngram_metrics import ROUGE\n",
    "from pprint import pprint\n",
    "\n",
    "# Initialize the metric\n",
    "r = ROUGE()\n",
    "\n",
    "# BERT evaluation\n",
    "score = r.calculate(\n",
    "    generated_texts=[\n",
    "        \"To make a pasta, cook the pasta.\",\n",
    "        \"To make a pasta, cook the pasta.\",\n",
    "    ],  # Generated text from LLM\n",
    "    reference_texts=[\n",
    "        \"Boil water, add pasta, cook for 10 minutes, and drain.\",\n",
    "        #   \"To make a pasta, cook the pasta.\"\n",
    "    ],  # Expected output\n",
    ")\n",
    "\n",
    "print(\"BERT score:\")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bfa852",
   "metadata": {},
   "source": [
    "Now, that the `llm_metrics` package is confirmed importable, let's continue by importing necessary modules from the library and other common packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9071b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core library imports\n",
    "from llm_metrics.ngram_metrics import BLEU, ROUGE\n",
    "from llm_metrics.semantic_similarity_metrics import BERTScore\n",
    "\n",
    "from pprint import pformat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9f999",
   "metadata": {},
   "source": [
    "### 2. Single Text Pair Processing\n",
    "\n",
    "Let's start with a simple example of comparing a generated text with a reference text using the `GenAIResultsComparator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f6d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "bleu = BLEU()\n",
    "rouge = ROUGE()\n",
    "bertscore = BERTScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edbfb6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score:\n",
      "0.056122223243057295\n",
      "\n",
      "ROUGE scores:\n",
      "{'rouge1': 0.3333333333333333, 'rouge2': 0.125, 'rougeL': 0.3333333333333333}\n",
      "\n",
      "BERTScore:\n",
      "{'f1': 0.8371803164482117, 'precision': 0.8371802568435669, 'recall': 0.8371802568435669}\n"
     ]
    }
   ],
   "source": [
    "# Example texts\n",
    "sentence_1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence_2 = \"A fast brown fox leaps over a sleepy canine\"\n",
    "\n",
    "# Calculate scores\n",
    "bleu_score = bleu.calculate(sentence_1, sentence_2)\n",
    "rouge_score = rouge.calculate(sentence_1, sentence_2)\n",
    "bert_score = bertscore.calculate(sentence_1, sentence_2)\n",
    "\n",
    "print(f\"BLEU score:\\n{bleu_score}\")\n",
    "print(f\"\\nROUGE scores:\\n{pformat(rouge_score, width=100)}\")\n",
    "print(f\"\\nBERTScore:\\n{pformat(bert_score, width=100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c86336",
   "metadata": {},
   "source": [
    "### 3. Batch Processing\n",
    "\n",
    "Now, let's move to batch processing, which allows you to handle multiple text pairs efficiently. This is particularly useful when you have a large dataset of generated texts and references.\n",
    "\n",
    "For simplicity, we will use lists of length 2 for both generated texts and references. In practice, these could be much longer lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49dcc64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch BLEU scores:\n",
      "0.03865275878469728\n",
      "\n",
      "Batch ROUGE scores:\n",
      "[{'rouge1': 0.3333333333333333, 'rouge2': 0.125, 'rougeL': 0.3333333333333333},\n",
      " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}]\n",
      "\n",
      "Batch BERTScores:\n",
      "[{'f1': 0.8371803164482117, 'precision': 0.8371802568435669, 'recall': 0.8371802568435669},\n",
      " {'f1': 0.6456650495529175, 'precision': 0.6858802437782288, 'recall': 0.6099045276641846}]\n"
     ]
    }
   ],
   "source": [
    "generated_texts = [\"The quick brown fox jumps over the lazy dog\", \"The cat chases the mouse\"]\n",
    "reference_texts = [\"A fast brown fox leaps over a sleepy canine\", \"A feline pursues a rodent\"]\n",
    "\n",
    "# Batch calculate scores\n",
    "bleu_scores = bleu.calculate(generated_texts, reference_texts)\n",
    "rouge_scores = rouge.calculate(generated_texts, reference_texts)\n",
    "bert_scores = bertscore.calculate(generated_texts, reference_texts)\n",
    "\n",
    "print(f\"\\nBatch BLEU scores:\\n{bleu_scores}\")\n",
    "print(f\"\\nBatch ROUGE scores:\\n{pformat(rouge_scores, width=100)}\")\n",
    "print(f\"\\nBatch BERTScores:\\n{pformat(bert_scores, width=100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424b7d4",
   "metadata": {},
   "source": [
    "### 4. Metric Customization\n",
    "\n",
    "You can customize the metrics used for evaluation by passing in various custom parameters to the metrics. This allows you to focus on specific aspects of text quality that are most relevant to your application.\n",
    "\n",
    "_Note:_ Each metric has it's own customization options. Refer to the documentation for details on available parameters for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6562470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize BLEU\n",
    "bleu = BLEU(n=3)  # Use 3-grams instead of default 4-grams\n",
    "\n",
    "# Customize ROUGE\n",
    "rouge = ROUGE(rouge_types=[\"rouge1\", \"rouge2\"], use_stemmer=True)\n",
    "\n",
    "# Customize BERTScore\n",
    "bert_score = BERTScore(model_type=\"bert-base-uncased\", num_layers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom BLEU score:\n",
      "0.056122223243057295\n",
      "\n",
      "Custom ROUGE scores:\n",
      "{'rouge1': 0.3333333333333333, 'rouge2': 0.125}\n",
      "\n",
      "Custom BERTScore:\n",
      "{'f1': 0.8371803164482117, 'precision': 0.8371802568435669, 'recall': 0.8371802568435669}\n"
     ]
    }
   ],
   "source": [
    "# Recalculate scores with custom settings\n",
    "bleu_score_custom = bleu.calculate(sentence_1, sentence_2)\n",
    "rouge_score_custom = rouge.calculate(sentence_1, sentence_2)\n",
    "bert_score_custom = bertscore.calculate(sentence_1, sentence_2)\n",
    "\n",
    "print(f\"\\nCustom BLEU score:\\n{bleu_score_custom}\")\n",
    "print(f\"\\nCustom ROUGE scores:\\n{pformat(rouge_score_custom, width=100)}\")\n",
    "print(f\"\\nCustom BERTScore:\\n{pformat(bert_score_custom, width=100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b071a8",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    "This notebook has demonstrated the core functionalities of the `GenAIResultsComparator` library. You've learned how to:\n",
    "\n",
    "The library is designed to be extensible, so you can also create your own custom metrics by inheriting from `BaseMetric`. For more advanced use cases, such as prompt-aware evaluation, check the `examples` folder in the library's repository.\n",
    "\n",
    "We encourage you to explore the library further with your own datasets and LLM outputs. Happy evaluating!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
