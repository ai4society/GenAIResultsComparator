{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right, #4b6cb7, #182848); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\">\n",
    "    <h1 style=\"color: white; margin: 0; font-size: 2.5em; font-weight: 700;\">GAICo: Audio Metrics</h1>\n",
    "    <p style=\"color: #e0e0e0; margin-top: 10px; font-style: italic; font-size: 1.2em; text-align: center;\">Evaluating AI-Generated Audio Content</p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ai4society/GenAIResultsComparator/blob/main/examples/example-audio.ipynb)\n",
    "\n",
    "This notebook demonstrates how to use **GAICo's** audio metrics for evaluating AI-generated audio outputs. We'll explore two specialized metrics designed for audio comparison:\n",
    "\n",
    "1. **AudioSNRNormalized**: Evaluates audio quality by calculating the Signal-to-Noise Ratio (SNR) between generated and reference audio\n",
    "2. **AudioSpectrogramDistance**: Compares spectral characteristics of audio signals using spectrogram-based distance measures\n",
    "\n",
    "**Use Cases:**\n",
    "- Evaluating text-to-speech (TTS) systems\n",
    "- Assessing music generation models\n",
    "- Comparing audio enhancement algorithms\n",
    "- Analyzing voice synthesis quality\n",
    "\n",
    "**What You'll Learn:**\n",
    "- How to use GAICo's audio metrics with various input formats\n",
    "- Understanding SNR and spectrogram-based audio evaluation\n",
    "- Batch processing of audio files\n",
    "- Integration with GAICo's Experiment class for comparative analysis\n",
    "- Visualization of audio metric results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Setup for Google Colab\n",
    "\n",
    "If you are running this notebook in Google Colab, uncomment and run the following cell to install the `gaico` package with audio dependencies.\n",
    "\n",
    "If you are running locally, you can skip this cell if you have already set up your environment according to the project's README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'gaico[audio]' -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup & Imports\n",
    "\n",
    "The cell below adjusts `sys.path` to find the gaico module if run from examples directory.\n",
    "This block is primarily for local execution from the `examples/` folder if gaico is not installed.\n",
    "\n",
    "After installation, you might need to restart the Colab runtime for the changes to take effect.\n",
    "(Runtime > Restart runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root directory (parent of 'examples')\n",
    "project_root = str(Path.cwd().parent) if \"examples\" in str(Path.cwd()) else str(Path.cwd())\n",
    "\n",
    "# Add project root to the system path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added project root to sys.path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, display\n",
    "import warnings\n",
    "\n",
    "# GAICo imports\n",
    "from gaico import Experiment\n",
    "from gaico.metrics.audio import AudioSNRNormalized, AudioSpectrogramDistance\n",
    "\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Audio Metrics\n",
    "\n",
    "### 1.1 Signal-to-Noise Ratio (SNR)\n",
    "\n",
    "The **AudioSNRNormalized** metric calculates how much the generated audio differs from the reference audio in terms of \"noise\" (unwanted differences). Higher SNR values indicate better quality.\n",
    "\n",
    "- **Perfect match**: SNR → ∞ (normalized to 1.0)\n",
    "- **Very noisy**: SNR → negative values (normalized to 0.0)\n",
    "\n",
    "### 1.2 Spectrogram Distance\n",
    "\n",
    "The **AudioSpectrogramDistance** metric compares the frequency content of audio signals over time. This is useful for:\n",
    "- Comparing timbral characteristics\n",
    "- Evaluating spectral fidelity\n",
    "- Assessing frequency preservation in audio generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Basic Usage with Synthetic Audio\n",
    "\n",
    "Let's start with simple synthetic audio examples to understand how the metrics work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic audio for testing\n",
    "sample_rate = 44100\n",
    "duration = 1.0  # seconds\n",
    "t = np.linspace(0, duration, int(sample_rate * duration), False)\n",
    "\n",
    "# Reference signal: 440 Hz sine wave (A4 note)\n",
    "reference_audio = np.sin(2 * np.pi * 440 * t).astype(np.float32)\n",
    "\n",
    "# Test signals with varying quality\n",
    "test_audio_identical = reference_audio.copy()\n",
    "test_audio_slight_noise = reference_audio + 0.1 * np.random.normal(0, 1, len(t)).astype(np.float32)\n",
    "test_audio_different_freq = np.sin(2 * np.pi * 880 * t).astype(np.float32)  # Octave higher\n",
    "test_audio_very_noisy = reference_audio + 0.5 * np.random.normal(0, 1, len(t)).astype(np.float32)\n",
    "\n",
    "print(\"Generated synthetic audio signals:\")\n",
    "print(\"- Reference: 440 Hz sine wave (A4 note)\")\n",
    "print(\"- Test 1: Identical to reference\")\n",
    "print(\"- Test 2: Slightly noisy version\")\n",
    "print(\"- Test 3: Different frequency (880 Hz)\")\n",
    "print(\"- Test 4: Very noisy version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Using AudioSNRNormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SNR metric\n",
    "snr_metric = AudioSNRNormalized(\n",
    "    snr_min=-20.0,  # Maps to score 0.0\n",
    "    snr_max=40.0,  # Maps to score 1.0\n",
    "    sample_rate=sample_rate,\n",
    ")\n",
    "\n",
    "# Calculate SNR scores\n",
    "snr_scores = {\n",
    "    \"Identical audio\": snr_metric.calculate(test_audio_identical, reference_audio),\n",
    "    \"Slight noise\": snr_metric.calculate(test_audio_slight_noise, reference_audio),\n",
    "    \"Different frequency\": snr_metric.calculate(test_audio_different_freq, reference_audio),\n",
    "    \"Very noisy\": snr_metric.calculate(test_audio_very_noisy, reference_audio),\n",
    "}\n",
    "\n",
    "print(\"\\nAudioSNRNormalized Results:\")\n",
    "print(\"==========================\")\n",
    "for name, score in snr_scores.items():\n",
    "    quality = (\n",
    "        \"perfect match\"\n",
    "        if score > 0.95\n",
    "        else \"good quality\"\n",
    "        if score > 0.7\n",
    "        else \"low quality\"\n",
    "        if score > 0.5\n",
    "        else \"poor match - different content\"\n",
    "    )\n",
    "    print(f\"{name:20}: {score:8.4f} ({quality})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using AudioSpectrogramDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spectrogram metrics with different distance types\n",
    "spec_metric_euclidean = AudioSpectrogramDistance(\n",
    "    n_fft=2048, hop_length=512, distance_type=\"euclidean\", sample_rate=sample_rate\n",
    ")\n",
    "\n",
    "spec_metric_cosine = AudioSpectrogramDistance(\n",
    "    n_fft=2048, hop_length=512, distance_type=\"cosine\", sample_rate=sample_rate\n",
    ")\n",
    "\n",
    "# Calculate spectrogram-based scores\n",
    "test_audios = [\n",
    "    (\"Identical audio\", test_audio_identical),\n",
    "    (\"Slight noise\", test_audio_slight_noise),\n",
    "    (\"Different frequency\", test_audio_different_freq),\n",
    "    (\"Very noisy\", test_audio_very_noisy),\n",
    "]\n",
    "\n",
    "print(\"\\nAudioSpectrogramDistance Results (Euclidean):\")\n",
    "print(\"============================================\")\n",
    "for name, audio in test_audios:\n",
    "    score = spec_metric_euclidean.calculate(audio, reference_audio)\n",
    "    print(f\"{name:20}: {score:8.4f} \")\n",
    "\n",
    "print(\"\\nAudioSpectrogramDistance Results (Cosine):\")\n",
    "print(\"=========================================\")\n",
    "for name, audio in test_audios:\n",
    "    score = spec_metric_cosine.calculate(audio, reference_audio)\n",
    "    print(f\"{name:20}: {score:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Batch Processing\n",
    "\n",
    "- GAICo's audio metrics support efficient batch processing. Let's demonstrate this with multiple audio samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of test audio with varying quality\n",
    "batch_size = 5\n",
    "noise_levels = [0.05, 0.15, 0.3, 0.5, 1.0]\n",
    "\n",
    "# Create batch of generated audio\n",
    "generated_batch = []\n",
    "model_names = []\n",
    "\n",
    "for i, noise_level in enumerate(noise_levels):\n",
    "    if i < 4:\n",
    "        # Add noise to reference\n",
    "        audio = reference_audio + noise_level * np.random.normal(0, 1, len(t)).astype(np.float32)\n",
    "    else:\n",
    "        # Last one is a completely different signal\n",
    "        audio = np.sin(2 * np.pi * 523.25 * t).astype(np.float32)  # C5 note\n",
    "\n",
    "    generated_batch.append(audio)\n",
    "\n",
    "    if i == 0:\n",
    "        model_names.append(\"High Quality\")\n",
    "    elif i == 1:\n",
    "        model_names.append(\"Medium Quality\")\n",
    "    elif i == 2:\n",
    "        model_names.append(\"Low Quality\")\n",
    "    elif i == 3:\n",
    "        model_names.append(\"White Noise\")\n",
    "    else:\n",
    "        model_names.append(\"Different\")\n",
    "\n",
    "# Create reference batch (same reference for all)\n",
    "reference_batch = [reference_audio] * batch_size\n",
    "\n",
    "# Batch calculate scores\n",
    "snr_batch_scores = snr_metric.calculate(generated_batch, reference_batch)\n",
    "spec_batch_scores = spec_metric_euclidean.calculate(generated_batch, reference_batch)\n",
    "\n",
    "# Create results DataFrame\n",
    "batch_results = pd.DataFrame(\n",
    "    {\"Model\": model_names, \"SNR Score\": snr_batch_scores, \"Spectrogram Score\": spec_batch_scores}\n",
    ")\n",
    "\n",
    "print(\"\\nBatch Processing Results:\")\n",
    "print(\"========================\")\n",
    "print(batch_results.to_string(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Batch Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot SNR scores\n",
    "bars1 = ax1.bar(\n",
    "    range(len(batch_results)), batch_results[\"SNR Score\"], color=\"skyblue\", edgecolor=\"navy\"\n",
    ")\n",
    "ax1.set_ylabel(\"Normalized SNR Score\")\n",
    "ax1.set_title(\"Audio Quality: SNR Comparison\")\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axhline(y=0.7, color=\"green\", linestyle=\"--\", alpha=0.5, label=\"Good Quality Threshold\")\n",
    "ax1.set_xticks(range(len(batch_results)))\n",
    "ax1.set_xticklabels(batch_results[\"Model\"], rotation=45, ha=\"right\")\n",
    "ax1.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 0.01,\n",
    "        f\"{height:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "# Plot Spectrogram scores\n",
    "bars2 = ax2.bar(\n",
    "    range(len(batch_results)),\n",
    "    batch_results[\"Spectrogram Score\"],\n",
    "    color=\"lightcoral\",\n",
    "    edgecolor=\"darkred\",\n",
    ")\n",
    "ax2.set_ylabel(\"Spectrogram Similarity Score\")\n",
    "ax2.set_title(\"Spectral Similarity Comparison\")\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.axhline(y=0.8, color=\"green\", linestyle=\"--\", alpha=0.5, label=\"High Similarity Threshold\")\n",
    "ax2.set_xticks(range(len(batch_results)))\n",
    "ax2.set_xticklabels(batch_results[\"Model\"], rotation=45, ha=\"right\")\n",
    "ax2.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars2):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 0.01,\n",
    "        f\"{height:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When no reference is provided, GAICo automatically uses the first generated audio as the   reference, similar to how text metrics work. This is useful for relative quality assessment and baseline comparisons.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔄 Missing Reference Handling:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Case 1: Single audio without reference\n",
    "print(\"1. Single audio without reference:\")\n",
    "single_score = snr_metric.calculate(test_audio_slight_noise, reference=None)\n",
    "print(f\"   Score (comparing with itself): {single_score:.3f}\")\n",
    "print(\"   → Always returns 1.0 for self-comparison\")\n",
    "\n",
    "# Case 2: Batch without reference - uses first as baseline\n",
    "print(\"\\n2. Batch processing without reference:\")\n",
    "batch_scores_no_ref = snr_metric.calculate(generated_batch, reference=None)\n",
    "print(f\"   Using first audio ('{model_names[0]}') as reference\")\n",
    "for model, score in zip(model_names, batch_scores_no_ref):\n",
    "    print(f\"   {model:15}: {score:.3f}\")\n",
    "\n",
    "print(\"\\n   → First model has score 1.0 (comparing with itself)\")\n",
    "print(f\"   → Other models compared against '{model_names[0]}'\")\n",
    "\n",
    "# Case 3: Visualize the comparison\n",
    "print(\"\\n3. Comparison: With vs Without Reference\")\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), sharey=True)\n",
    "fig.suptitle(\"Comparison: With vs. Without Explicit Reference\", fontsize=16)\n",
    "\n",
    "# With explicit reference\n",
    "with_ref_scores = snr_metric.calculate(generated_batch, reference_batch)\n",
    "ax1.bar(model_names, with_ref_scores, color=\"skyblue\", edgecolor=\"navy\")\n",
    "ax1.set_title(\"With Explicit Reference\", fontsize=14)\n",
    "ax1.set_ylabel(\"SNR Score\")\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Without reference (first as baseline)\n",
    "bars2 = ax2.bar(model_names, batch_scores_no_ref, color=\"lightcoral\", edgecolor=\"darkred\")\n",
    "ax2.set_title(\"Without Reference (First as Baseline)\", fontsize=14)\n",
    "ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Highlight the baseline\n",
    "bars2[0].set_color(\"gold\")\n",
    "bars2[0].set_edgecolor(\"orange\")\n",
    "ax2.text(\n",
    "    0,\n",
    "    batch_scores_no_ref[0] / 2,\n",
    "    \"Baseline\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontweight=\"bold\",\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, score) in enumerate(zip(ax1.patches, with_ref_scores)):\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2, score + 0.02, f\"{score:.2f}\", ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "for i, (bar, score) in enumerate(zip(ax2.patches, batch_scores_no_ref)):\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2, score + 0.02, f\"{score:.2f}\", ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Key Insights:\")\n",
    "print(\"   • Missing reference enables relative quality assessment.\")\n",
    "print(\"   • The first audio in the batch becomes the quality benchmark.\")\n",
    "print(\n",
    "    \"   • This is useful for ranking multiple generations when a single ground truth is unavailable.\"\n",
    ")\n",
    "print(\"   • The behavior is consistent with GAICo's text and structured data metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Real-World Example - Text-to-Speech Evaluation\n",
    "\n",
    "Let's simulate a scenario where we're evaluating different TTS models. We'll create audio samples with characteristics typical of TTS outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate speech-like signals using formants\n",
    "def create_speech_like_signal(duration, sample_rate, formants, amplitudes, add_prosody=True):\n",
    "    \"\"\"Create a simplified speech-like signal with formants.\"\"\"\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), False)\n",
    "    signal = np.zeros_like(t)\n",
    "\n",
    "    for formant, amp in zip(formants, amplitudes):\n",
    "        if add_prosody:\n",
    "            # Add slight frequency modulation for prosody\n",
    "            freq_mod = 1 + 0.05 * np.sin(2 * np.pi * 3 * t)\n",
    "            signal += amp * np.sin(2 * np.pi * formant * freq_mod * t)\n",
    "        else:\n",
    "            signal += amp * np.sin(2 * np.pi * formant * t)\n",
    "\n",
    "    # Add envelope for more natural sound\n",
    "    envelope = np.exp(-t * 0.5) * (1 - np.exp(-t * 20))\n",
    "    return (signal * envelope).astype(np.float32)\n",
    "\n",
    "\n",
    "# Reference \"speech\" - clean signal\n",
    "formants = [700, 1220, 2600]  # Simplified vowel formants\n",
    "amplitudes = [1.0, 0.5, 0.3]\n",
    "reference_speech = create_speech_like_signal(\n",
    "    2.0, sample_rate, formants, amplitudes, add_prosody=True\n",
    ")\n",
    "\n",
    "# TTS Model outputs with varying quality\n",
    "tts_outputs = {\n",
    "    \"Model A (Premium)\": reference_speech\n",
    "    + 0.02 * np.random.normal(0, 1, len(reference_speech)).astype(np.float32),\n",
    "    \"Model B (Standard)\": create_speech_like_signal(\n",
    "        2.0,\n",
    "        sample_rate,\n",
    "        [690, 1200, 2550],  # Slightly shifted formants\n",
    "        [0.9, 0.4, 0.25],\n",
    "        add_prosody=True,\n",
    "    )\n",
    "    + 0.05 * np.random.normal(0, 1, len(reference_speech)).astype(np.float32),\n",
    "    \"Model C (Basic)\": create_speech_like_signal(\n",
    "        2.0,\n",
    "        sample_rate,\n",
    "        [680, 1180, 2500],  # More shifted formants\n",
    "        [0.8, 0.3, 0.2],\n",
    "        add_prosody=True,\n",
    "    )\n",
    "    + 0.1 * np.random.normal(0, 1, len(reference_speech)).astype(np.float32),\n",
    "    \"Model D (Robotic)\": create_speech_like_signal(\n",
    "        2.0, sample_rate, formants, [0.7, 0.3, 0.1], add_prosody=False\n",
    "    ),  # No prosody = robotic\n",
    "}\n",
    "\n",
    "print(\"Simulated TTS evaluation scenario:\")\n",
    "print(\"- Reference: Clean speech-like signal\")\n",
    "print(\"- Model A: High-quality TTS (minimal artifacts)\")\n",
    "print(\"- Model B: Standard TTS (some spectral artifacts)\")\n",
    "print(\"- Model C: Basic TTS (noticeable artifacts)\")\n",
    "print(\"- Model D: Robotic TTS (monotone, poor prosody)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive audio playback for demonstration\n",
    "print(\"\\n🔊 Listen to the TTS Model Outputs\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Click play to hear each model's output:\\n\")\n",
    "\n",
    "# Reference audio\n",
    "print(\"📌 REFERENCE SPEECH (Ground Truth):\")\n",
    "display(Audio(reference_speech, rate=sample_rate, autoplay=False))\n",
    "\n",
    "# Model outputs\n",
    "for model_name, audio in tts_outputs.items():\n",
    "    print(f\"\\n🎤 {model_name}:\")\n",
    "    display(Audio(audio, rate=sample_rate, autoplay=False))\n",
    "\n",
    "    # Show quick stats\n",
    "    snr_score = snr_metric.calculate(audio, reference_speech)\n",
    "    print(\n",
    "        f\"   → SNR Score: {snr_score:.3f} | Quality: {'Excellent' if snr_score > 0.9 else 'Good' if snr_score > 0.7 else 'Fair' if snr_score > 0.5 else 'Poor'}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Using the Experiment Class\n",
    "\n",
    "GAICo's `Experiment` class provides a streamlined workflow for comparing multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Experiment instance\n",
    "exp = Experiment(\n",
    "    llm_responses=tts_outputs,  # Using audio data instead of text\n",
    "    reference_answer=reference_speech,\n",
    ")\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = Path(\"data/audio\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Compare using audio metrics\n",
    "results_df = exp.compare(\n",
    "    metrics=[\"AudioSNR\", \"AudioSpectrogramDistance\"],\n",
    "    plot=True,\n",
    "    output_csv_path=output_dir / \"tts_evaluation.csv\",\n",
    "    custom_thresholds={\n",
    "        \"AudioSNR\": 0.7,  # Good quality threshold\n",
    "        \"AudioSpectrogramDistance\": 0.8,  # High similarity threshold\n",
    "    },\n",
    "    plot_title_suffix=\"for TTS Evaluation\",\n",
    ")\n",
    "\n",
    "print(\"\\nTTS Evaluation Results:\")\n",
    "print(\"======================\")\n",
    "# Displaying with higher precision for clarity\n",
    "with pd.option_context(\"display.precision\", 4):\n",
    "    print(results_df.pivot(index=\"model_name\", columns=\"metric_name\", values=\"score\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Working with Audio Files\n",
    "\n",
    "GAICo's audio metrics can also work directly with audio file paths. Let's demonstrate this capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary audio files for demonstration\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "\n",
    "# Create a temporary directory\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Save audio files\n",
    "    ref_path = os.path.join(temp_dir, \"reference.wav\")\n",
    "    gen_good_path = os.path.join(temp_dir, \"generated_good.wav\")\n",
    "    gen_poor_path = os.path.join(temp_dir, \"generated_poor.wav\")\n",
    "\n",
    "    # Write audio files\n",
    "    sf.write(ref_path, reference_audio, sample_rate)\n",
    "    sf.write(gen_good_path, test_audio_slight_noise, sample_rate)\n",
    "    sf.write(gen_poor_path, test_audio_very_noisy, sample_rate)\n",
    "\n",
    "    print(\"Created temporary audio files:\")\n",
    "    print(\"- reference.wav\")\n",
    "    print(\"- generated_good.wav\")\n",
    "    print(\"- generated_poor.wav\")\n",
    "\n",
    "    # Use file paths with metrics\n",
    "    snr_score_good = snr_metric.calculate(gen_good_path, ref_path)\n",
    "    spec_score_good = spec_metric_euclidean.calculate(gen_good_path, ref_path)\n",
    "\n",
    "    snr_score_poor = snr_metric.calculate(gen_poor_path, ref_path)\n",
    "    spec_score_poor = spec_metric_euclidean.calculate(gen_poor_path, ref_path)\n",
    "\n",
    "    print(\"\\nFile-based Audio Comparison:\")\n",
    "    print(\"============================\")\n",
    "    print(\"Good Quality Audio:\")\n",
    "    print(f\"  SNR Score: {snr_score_good:.4f}\")\n",
    "    print(f\"  Spectrogram Score: {spec_score_good:.4f}\")\n",
    "    print(\"\\nPoor Quality Audio:\")\n",
    "    print(f\"  SNR Score: {snr_score_poor:.4f}\")\n",
    "    print(f\"  Spectrogram Score: {spec_score_poor:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Configuration\n",
    "\n",
    "Both audio metrics offer various configuration options for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different SNR configurations\n",
    "snr_configs = [\n",
    "    (\"Standard range (-20 to 40 dB)\", AudioSNRNormalized(snr_min=-20, snr_max=40)),\n",
    "    (\"Strict range (-10 to 20 dB)\", AudioSNRNormalized(snr_min=-10, snr_max=20)),\n",
    "    (\"Lenient range (-30 to 60 dB)\", AudioSNRNormalized(snr_min=-30, snr_max=60)),\n",
    "]\n",
    "\n",
    "# Test different spectrogram configurations\n",
    "spec_distance_types = [\n",
    "    (\"Euclidean\", AudioSpectrogramDistance(distance_type=\"euclidean\")),\n",
    "    (\"Cosine\", AudioSpectrogramDistance(distance_type=\"cosine\")),\n",
    "    (\"Correlation\", AudioSpectrogramDistance(distance_type=\"correlation\")),\n",
    "]\n",
    "\n",
    "spec_fft_sizes = [\n",
    "    (\"1024 samples\", AudioSpectrogramDistance(n_fft=1024)),\n",
    "    (\"2048 samples\", AudioSpectrogramDistance(n_fft=2048)),\n",
    "    (\"4096 samples\", AudioSpectrogramDistance(n_fft=4096)),\n",
    "]\n",
    "\n",
    "print(\"\\nConfiguration Comparison:\")\n",
    "print(\"========================\")\n",
    "\n",
    "print(\"\\nSNR Metric Configurations:\")\n",
    "for config_name, metric in snr_configs:\n",
    "    score = metric.calculate(test_audio_slight_noise, reference_audio)\n",
    "    print(f\"{config_name:30}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nSpectrogram Configurations:\")\n",
    "print(\"Distance Type Comparison:\")\n",
    "for config_name, metric in spec_distance_types:\n",
    "    score = metric.calculate(test_audio_slight_noise, reference_audio)\n",
    "    print(f\"  {config_name:12}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nFFT Size Comparison:\")\n",
    "for config_name, metric in spec_fft_sizes:\n",
    "    score = metric.calculate(test_audio_slight_noise, reference_audio)\n",
    "    print(f\"  {config_name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Music Generation Evaluation\n",
    "\n",
    "Let's demonstrate how these metrics can be used to evaluate music generation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple musical phrase (C major arpeggio)\n",
    "def create_musical_phrase(frequencies, duration_per_note, sample_rate, add_harmonics=True):\n",
    "    \"\"\"Create a musical phrase from a sequence of frequencies.\"\"\"\n",
    "    phrase = []\n",
    "    for freq in frequencies:\n",
    "        t = np.linspace(0, duration_per_note, int(sample_rate * duration_per_note), False)\n",
    "        note = np.sin(2 * np.pi * freq * t)\n",
    "\n",
    "        if add_harmonics:\n",
    "            # Add harmonics for richer sound\n",
    "            note += 0.3 * np.sin(2 * np.pi * freq * 2 * t)  # 2nd harmonic\n",
    "            note += 0.1 * np.sin(2 * np.pi * freq * 3 * t)  # 3rd harmonic\n",
    "\n",
    "        # Add envelope\n",
    "        envelope = np.exp(-t * 2)\n",
    "        phrase.append(note * envelope)\n",
    "\n",
    "    return np.concatenate(phrase).astype(np.float32)\n",
    "\n",
    "\n",
    "# Reference musical phrase (C major arpeggio)\n",
    "c_major_freqs = [261.63, 329.63, 392.00, 523.25]  # C4, E4, G4, C5\n",
    "reference_music = create_musical_phrase(c_major_freqs, 0.5, sample_rate, add_harmonics=True)\n",
    "\n",
    "# Simulated music generation outputs\n",
    "music_generations = {\n",
    "    \"Perfect Copy\": reference_music.copy(),\n",
    "    \"Style Transfer\": create_musical_phrase(\n",
    "        [261.63, 329.63, 392.00, 523.25],  # Same notes\n",
    "        0.5,\n",
    "        sample_rate,\n",
    "        add_harmonics=False,  # Different timbre\n",
    "    )\n",
    "    + 0.05 * np.random.normal(0, 1, len(reference_music)).astype(np.float32),\n",
    "    \"Genre Variation\": create_musical_phrase(\n",
    "        [261.63, 311.13, 392.00, 466.16],  # C minor variation\n",
    "        0.5,\n",
    "        sample_rate,\n",
    "        add_harmonics=True,\n",
    "    ),\n",
    "    \"Amateur Generation\": create_musical_phrase(\n",
    "        [250, 320, 380, 510],  # Slightly off-pitch\n",
    "        0.45,\n",
    "        sample_rate,\n",
    "        add_harmonics=False,  # Also different timing\n",
    "    )[: len(reference_music)],  # Truncate to match length\n",
    "}\n",
    "\n",
    "# Evaluate with multiple metric configurations\n",
    "music_results = []\n",
    "for model_name, generated_music in music_generations.items():\n",
    "    results = {\n",
    "        \"Model\": model_name,\n",
    "        \"AudioSNRNormalized\": snr_metric.calculate(generated_music, reference_music),\n",
    "        \"AudioSpectrogramDist_euclidean\": spec_metric_euclidean.calculate(\n",
    "            generated_music, reference_music\n",
    "        ),\n",
    "        \"AudioSpectrogramDist_cosine\": spec_metric_cosine.calculate(\n",
    "            generated_music, reference_music\n",
    "        ),\n",
    "    }\n",
    "    music_results.append(results)\n",
    "\n",
    "music_df = pd.DataFrame(music_results)\n",
    "music_df.set_index(\"Model\", inplace=True)\n",
    "\n",
    "print(\"\\nMusic Generation Evaluation:\")\n",
    "print(\"===========================\")\n",
    "print(music_df)\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "music_df.plot(kind=\"bar\", ax=ax, width=0.8)\n",
    "ax.set_title(\"Music Generation Quality Comparison\", fontsize=16, pad=20)\n",
    "ax.set_ylabel(\"Score (0-1)\", fontsize=12)\n",
    "ax.set_xlabel(\"Music Generation Models\", fontsize=12)\n",
    "ax.legend(title=\"Metrics\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.3f\", rotation=90, fontsize=8, padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Error Handling and Edge Cases\n",
    "\n",
    "GAICo's audio metrics include comprehensive error handling. Let's explore some edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import soundfile as sf\n",
    "\n",
    "print(\"\\nError Handling Examples:\")\n",
    "print(\"=======================\")\n",
    "\n",
    "# 1. Empty audio\n",
    "try:\n",
    "    print(\"\\n1. Empty audio array:\")\n",
    "    score = snr_metric.calculate(np.array([]), reference_audio)\n",
    "except ValueError as e:\n",
    "    print(f\"   Error caught: {e}\")\n",
    "\n",
    "# 2. Mismatched lengths (automatically handled)\n",
    "print(\"\\n2. Mismatched lengths (handled automatically):\")\n",
    "short_audio = reference_audio[: len(reference_audio) // 2]\n",
    "score = snr_metric.calculate(reference_audio, short_audio)\n",
    "print(f\"   Score with auto-truncation: {score:.4f}\")\n",
    "\n",
    "# 3. Invalid file path\n",
    "try:\n",
    "    print(\"\\n3. Invalid file path:\")\n",
    "    score = snr_metric.calculate(\"/nonexistent/audio.wav\", reference_audio)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"   Error caught: {e}\")\n",
    "\n",
    "# 4. Different sample rates (warning issued)\n",
    "print(\"\\n4. Different sample rates (handling demonstration):\")\n",
    "# Create audio at a different sample rate\n",
    "sr_22k = 22050\n",
    "t_22k = np.linspace(0, 1.0, sr_22k, False)\n",
    "audio_22k = np.sin(2 * np.pi * 440 * t_22k).astype(np.float32)\n",
    "\n",
    "# Use file paths to test with different sample rates\n",
    "with (\n",
    "    tempfile.NamedTemporaryFile(suffix=\".wav\") as gen_tmp_file,\n",
    "    tempfile.NamedTemporaryFile(suffix=\".wav\") as ref_tmp_file,\n",
    "):\n",
    "    # Save the generated audio with its actual sample rate (22050 Hz)\n",
    "    sf.write(gen_tmp_file.name, audio_22k, sr_22k)\n",
    "    # Save the reference audio with its actual sample rate (44100 Hz)\n",
    "    sf.write(ref_tmp_file.name, reference_audio, sample_rate)\n",
    "\n",
    "    # Process files with different sample rates\n",
    "    try:\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.simplefilter(\"always\")\n",
    "            score = snr_metric.calculate(gen_tmp_file.name, ref_tmp_file.name)\n",
    "            print(f\"   Score with auto-resampling: {score:.4f}\")\n",
    "\n",
    "            if len(w) > 0 and \"Sample rates differ\" in str(w[-1].message):\n",
    "                print(f\"   ✅ Warning issued: {w[-1].message}\")\n",
    "            else:\n",
    "                print(\"   ✓ Sample rate differences handled silently (no warning)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {e}\")\n",
    "\n",
    "# 5. Very short audio\n",
    "try:\n",
    "    print(\"\\n5. Very short audio (too short for spectrogram):\")\n",
    "    very_short = np.array([0.1, 0.2, 0.3, 0.4, 0.5], dtype=np.float32)\n",
    "    score = spec_metric_euclidean.calculate(very_short, reference_audio)\n",
    "except ValueError as e:\n",
    "    print(f\"   Error caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored GAICo's audio metrics:\n",
    "\n",
    "1. **AudioSNRNormalized**: Measures signal quality by comparing noise levels\n",
    "   - Best for: Overall quality assessment, noise evaluation\n",
    "   - Configuration: Adjustable SNR range for different applications\n",
    "\n",
    "2. **AudioSpectrogramDistance**: Compares spectral characteristics\n",
    "   - Best for: Timbre comparison, frequency content analysis\n",
    "   - Configuration: Multiple distance types, adjustable FFT parameters\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Both metrics support various input formats (numpy arrays, file paths, lists)\n",
    "- Automatic handling of common issues (sample rate differences, length mismatches)\n",
    "- Seamless integration with GAICo's Experiment class for multi-model comparison\n",
    "- Normalized outputs (0-1) for consistency with other GAICo metrics\n",
    "\n",
    "**When to Use Which Metric:**\n",
    "- Use **SNR** when you care about overall signal fidelity and noise levels\n",
    "- Use **Spectrogram Distance** when spectral characteristics and timbre are important\n",
    "- Consider using both for comprehensive audio quality assessment\n",
    "\n",
    "For more information on GAICo and its other metrics, visit the [documentation](https://ai4society.github.io/projects/GenAIResultsComparator)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
