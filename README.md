# GenAIResultsComparator

_GenAIResultsComparator_ is a Python library for evaluation metrics to
compare generated texts with their ground truth versions.
This library is particularly useful for evaluating outputs generated by
Large Language Models (LLM).

Inspiration for the library and evaluation metrics was taken from [Microsoft's
article on evaluating LLM-generated content](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics).
In the article, Microsoft describes 3 categories of evaluation metrics: (1) Reference-based metrics, (2) Reference-free metrics, and (3) LLM-based metrics. _The library currently supports reference-based metrics._

The library provides a set of metrics for evaluating **2 text strings** as inputs.
Each metric is implemented as a Python class with **two methods**:
- `calculate()`: Computes the metric for a single pair of texts
- `batch_calculate()`: Efficiently processes multiple pairs of texts using vectorized operations and batch processing

All outputs are normalized to a scale of 0 to 1, where 1 indicates a perfect match between the two texts.

**_Note:_** While the library can be used to compare strings, and we demonstrate this in the examples below, it's main purpose is to be used with generated texts from LLMs. An example of this can be found in the [examples/llm_aware_metrics](examples/llm_aware_metrics) folder.

## Quick Start

_Detailed examples can be found in the [examples](examples) folder._

Here's a simple example of how to use GenAIResultsComparator:

```python
from llm_metrics import BLEU, ROUGE, BERTScore

# Initialize metrics
bleu = BLEU()
rouge = ROUGE()
bert_score = BERTScore()

# Example texts
sentence_1 = "The quick brown fox jumps over the lazy dog"
sentence_2 = "A fast brown fox leaps over a sleepy canine"

# Calculate scores
bleu_score = bleu.calculate(sentence_1, sentence_2)
rouge_score = rouge.calculate(sentence_1, sentence_2)
bert_score = bert_score.calculate(sentence_1, sentence_2)

print(f"BLEU score: {bleu_score}")
print(f"ROUGE scores: {rouge_score}")
print(f"BERTScore: {bert_score}")

# For batch processing
generated_texts = [
    "The quick brown fox jumps over the lazy dog",
    "The cat chases the mouse"
]
reference_texts = [
    "A fast brown fox leaps over a sleepy canine",
    "A feline pursues a rodent"
]

# Batch calculate scores
bleu_scores = bleu.batch_calculate(generated_texts, reference_texts)
rouge_scores = rouge.batch_calculate(generated_texts, reference_texts)
bert_scores = bert_score.batch_calculate(generated_texts, reference_texts)
```

Furthermore, each metric can be customized during initialization:

```python
# Customize BLEU
bleu = BLEU(n=3)  # Use 3-grams instead of default 4-grams

# Customize ROUGE
rouge = ROUGE(rouge_types=['rouge1', 'rouge2'], use_stemmer=True)

# Customize BERTScore
bert_score = BERTScore(model_type='bert-base-uncased', num_layers=8)
```

Lastly, for advanced users, all metrics support additional parameters:

```python
# Pass additional parameters during calculation
bleu_score = bleu.calculate(text1, text2, additional_params={
    'smoothing_function': custom_smoothing
})
```

For detailed usage and more examples, please refer to the [Detailed Usage](#detailed-usage) section.

## Table of Contents
- [Features](#features)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [TODOs](#todos)
- [Development](#development)
- [Contributing](#contributing)
- [Detailed Usage](#detailed-usage)
- [License](#license)
- [Acknowledgments](#acknowledgments)
- [Contact](#contact)

## Features

- Implements various metrics for text comparison:
  - N-gram-based metrics (BLEU, ROUGE, JS divergence)
  - Text similarity metrics (Jaccard, Cosine, Levenshtein)
  - Semantic similarity metrics (BERTScore)
- Supports batch processing for efficient computation
- Optimized for different input types (lists, numpy arrays, pandas Series)
- Extendable architecture for easy addition of new metrics
- Testing suite

## Installation

Currently, LLM Metrics is not available on PyPI. To use it, you'll need to clone the repository and set up the environment using Poetry.

1. First, make sure you have Poetry installed. If not, you can install it by following the instructions on the [official Poetry website](https://python-poetry.org/docs/#installation).

2. Clone the repository:
   ```bash
   git clone https://github.com/ai4society/GenAIResultsComparator.git
   cd GenAIResultsComparator
   ```

3. Install the dependencies using Poetry:
   ```bash
   poetry install
   ```

4. Activate the virtual environment:
   ```bash
   poetry shell
   ```

Now you're ready to use LLM Metrics!

## Project Structure

The project structure is as follows:
```shell
.
├── README.md
├── LICENSE
├── .gitignore
├── poetry.lock
├── pyproject.toml
├── .pre-commit-config.yaml
├── llm_metrics/  # Contains the library code
├── examples/  # Contains example scripts
└── tests/  # Contains test scripts
```

## TODOs
- Add a more comprehensive test suite
- Add more metrics from the [Microsoft article](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics)

## Development

To set up the development environment:

1. Clone the repository:
   ```
   git clone https://github.com/ai4society/GenAIResultsComparator.git
   cd GenAIResultsComparator
   ```

2. Install dependencies using Poetry:
   ```
   poetry install
   ```

3. Run tests:
   ```
   poetry run pytest tests/
   ```

### Code Style

We use `pre-commit` hooks to maintain code quality and consistency. The configuration for these hooks is in the `.pre-commit-config.yaml` file. These hooks run automatically on `git commit`, but you can also run them manually:

```
pre-commit run --all-files
```

Our pre-commit hooks include:
- Code formatting with Black
- Import sorting with isort
- Linting with flake8
- Type checking with mypy

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/FeatureName`)
3. Commit your changes (`git commit -m 'Add some FeatureName'`)
4. Push to the branch (`git push origin feature/FeatureName`)
5. Open a Pull Request

Please ensure that your code passes all tests and adheres to our code style guidelines (enforced by pre-commit hooks) before submitting a pull request.

## Detailed Usage

### N-gram-based Metrics

#### BLEU (Bilingual Evaluation Understudy)

```python
from llm_metrics import BLEU

bleu = BLEU(n=4)  # n is the maximum n-gram order
score = bleu.calculate(generated_text, reference_text)
```

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

```python
from llm_metrics import ROUGE

rouge = ROUGE(rouge_types=['rouge1', 'rouge2', 'rougeL'])
scores = rouge.calculate(generated_text, reference_text)
```

#### JS Divergence

```python
from llm_metrics import JSDivergence

js_div = JSDivergence()
score = js_div.calculate(generated_text, reference_text)
```

### Text Similarity Metrics

#### Jaccard Similarity

```python
from llm_metrics import JaccardSimilarity

jaccard = JaccardSimilarity()
score = jaccard.calculate(generated_text, reference_text)
```

#### Cosine Similarity

```python
from llm_metrics import CosineSimilarity

cosine = CosineSimilarity()
score = cosine.calculate(generated_text, reference_text)
```

#### Levenshtein Distance

```python
from llm_metrics import LevenshteinDistance

levenshtein = LevenshteinDistance()
distance = levenshtein.calculate(generated_text, reference_text)
```

### Semantic Similarity Metrics

#### BERTScore

```python
from llm_metrics import BERTScore

bert_score = BERTScore(model_type="bert-base-uncased")
score = bert_score.calculate(generated_text, reference_text)
```

### Batch Processing

All metrics support batch processing for efficient computation on multiple texts:

```python
generated_texts = ["Text 1", "Text 2", "Text 3"]
reference_texts = ["Ref 1", "Ref 2", "Ref 3"]

scores = metric.batch_calculate(generated_texts, reference_texts)
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- This library uses several open-source packages including NLTK, scikit-learn, and others.
- Special thanks to the creators and maintainers of the implemented metrics.

## Contact

If you have any questions, feel free to reach out to us at [ai4societyteam@gmail.com](mailto:ai4societyteam@gmail.com).
