# GAICo: GenAI Results Comparator

_GAICo_ is a Python library for evaluation metrics to compare generated texts with their ground truth versions.
This library is particularly useful for evaluating outputs generated by Large Language Models (LLM).

## Quick Start

_Detailed examples can be found in the [examples](examples) folder._

```python
from gaico.semantic_similarity_metrics import BERTScore

# Initialize the metric
bert_score = BERTScore()

# BERT evaluation
score = bert_score.calculate(
  generated_texts=["To make a pasta, cook the pasta."],  # Generated text from LLM
  reference_texts=["Boil water, add pasta, cook for 10 minutes, and drain."],  # Expected output
)

print(f"BERT score: {score}")
```

Output:

```shell
> BERT score: [{'precision': 0.6500923037528992, 'recall': 0.5713468790054321, 'f1': 0.6081812977790833}]
```

Using `BERTScore`, we get three scores:

- Precision: 0.65 - Indicates how many of the generated tokens matched the reference tokens.
- Recall: 0.57 - Indicates how many of the reference tokens were captured by the generated text.
- F1 Score: 0.61 - The harmonic mean of precision and recall, providing a balance between the two.

These scores indicate that:

- The generated text has a good match with the reference text, but there is room for improvement.
- The precision is higher than recall, suggesting that the generated text is more focused on relevant tokens but misses some important ones from the reference.

## Description

The library provides a set of metrics for evaluating **2 text strings as inputs**. **Outputs are on a scale of 0 to 1** (normalized), where 1 indicates a perfect match between the two texts.

**_Class Structure:_** All metrics are implemented as classes, and they can be easily extended to add new metrics. The metrics start with the `BaseMetric` class under the `gaico/base.py` file.

Each metric class inherits from this base class and is implemented with **just one required method**: `calculate()`.

This `calculate()` method takes two parameters:

- `generated_texts`: Either a string or a Iterables of strings representing the texts generated by an LLM.
- `reference_texts`: Either a string or a Iterables of strings representing the expected or reference texts.

If the inputs are Iterables (lists, Numpy arrays, etc.), then the method assumes that there exists a one-to-one mapping between the generated texts and reference texts, meaning that the first generated text corresponds to the first reference text, and so on.

**_Note:_** While the library can be used to compare strings, it's main purpose is to aid with comparing results from various LLMs.

**_Inspiration_** for the library and evaluation metrics was taken from [Microsoft's
article on evaluating LLM-generated content](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics). In the article, Microsoft describes 3 categories of evaluation metrics: **(1)** Reference-based metrics, **(2)** Reference-free metrics, and **(3)** LLM-based metrics. _The library currently supports reference-based metrics._


<p align="center">
  <img src="gaico.drawio.png" alt="GAICo Overview">
</p>
<p align="center">
  <em>Overview of the workflow supported by the <i>GAICo</i> library</em>
</p>

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [Development](#development)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)
- [Contact](#contact)

## Features

- Implements various metrics for text comparison:
  - N-gram-based metrics (BLEU, ROUGE, JS divergence)
  - Text similarity metrics (Jaccard, Cosine, Levenshtein, Sequence Matcher)
  - Semantic similarity metrics (BERTScore)
- Supports batch processing for efficient computation
- Optimized for different input types (lists, numpy arrays, pandas Series)
- Extendable architecture for easy addition of new metrics
- Testing suite

## Installation

Currently, LLM Metrics is not available on PyPI. To use it, you'll need to clone the repository and set up the environment using UV.

1. First, make sure you have UV installed. If not, you can install it by following the instructions on the [official UV website](https://docs.astral.sh/uv/#installation).

2. Clone the repository:

   ```shell
   git clone https://github.com/ai4society/GenAIResultsComparator.git
   cd GenAIResultsComparator
   ```

3. Ensure the dependencies are installed by creating a virtual env. (python 3.12 is recommended):

   ```shell
   uv venv
   uv sync
   ```

4. (Optional) Activate the virtual environment (doing this avoids prepending `uv run` to any proceeding commands):
   ```shell
   source .venv/bin/activate
   ```

_If you don't want to use `uv`,_ you can install the dependencies with the following commands:

```shell
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

However note that the `requirements.txt` is generated automatically with the pre-commit file and might not include all the dependencies (in such case, a manual pip install might be needed).

Now you're ready to use LLM Metrics!

## Project Structure

The project structure is as follows:

```shell
.
├── README.md
├── LICENSE
├── .gitignore
├── uv.lock
├── pyproject.toml
├── .pre-commit-config.yaml
├── gaico/  # Contains the library code
├── examples/     # Contains example scripts
└── tests/        # Contains test scripts
```

### Code Style

We use `pre-commit` hooks to maintain code quality and consistency. The configuration for these hooks is in the `.pre-commit-config.yaml` file. These hooks run automatically on `git commit`, but you can also run them manually:

```
pre-commit run --all-files
```

## Running Tests

Navigate to the project root in your terminal and run:

```bash
uv run pytest
```

Or, for more verbose output:

```bash
uv run pytest -v
```


To skip the slow BERTScore tests:

```bash
uv run pytest -m "not bertscore"
```

To run only the slow BERTScore tests:

```bash
uv run pytest -m bertscore
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/FeatureName`)
3. Commit your changes (`git commit -m 'Add some FeatureName'`)
4. Push to the branch (`git push origin feature/FeatureName`)
5. Open a Pull Request

Please ensure that your code passes all tests and adheres to our code style guidelines (enforced by pre-commit hooks) before submitting a pull request.

## Acknowledgments

- This library uses several open-source packages including NLTK, scikit-learn, and others.
- Special thanks to the creators and maintainers of the implemented metrics.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contact

If you have any questions, feel free to reach out to us at [ai4societyteam@gmail.com](mailto:ai4societyteam@gmail.com).
