# GenAIResultsComparator

_GenAIResultsComparator_ is a Python library for evaluation metrics to
compare generated texts with their ground truth versions.
This library is particularly useful for evaluating outputs generated by
Large Language Models (LLM).

Inspiration for the library and evaluation metrics was taken from [Microsoft's
article on evaluating LLM-generated content](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics).
In the article, Microsoft describes 3 categories of evaluation metrics: (1) Reference-based metrics, (2) Reference-free metrics, and (3) LLM-based metrics. _The library currently supports reference-based metrics._

The library provides a set of metrics for evaluating **2 text strings** as inputs.
Each metric is implemented as a Python class with **two methods**:
- `calculate()`: Computes the metric for a single pair of texts
- `batch_calculate()`: Efficiently processes multiple pairs of texts using vectorized operations and batch processing

**_Note:_** While the library can be used to compare strings, and we demonstrate this in the examples below, it's main purpose is to be used with generated texts from LLMs.

## Quick Start

_Detailed examples can be found in the [examples](examples) folder._

Here's a simple example of how to use GenAIResultsComparator:

```python
from llm_metrics import BLEU, ROUGE, BERTScore

# Initialize metrics
bleu = BLEU()
rouge = ROUGE()
bert_score = BERTScore()

# Example texts
sentence_1 = "The quick brown fox jumps over the lazy dog"
sentence_2 = "A fast brown fox leaps over a sleepy canine"

# Calculate scores
bleu_score = bleu.calculate(sentence_1, sentence_2)
rouge_score = rouge.calculate(sentence_1, sentence_2)
bert_score = bert_score.calculate(sentence_1, sentence_2)

print(f"BLEU score: {bleu_score}")
print(f"ROUGE scores: {rouge_score}")
print(f"BERTScore: {bert_score}")

# For batch processing
generated_texts = [
    "The quick brown fox jumps over the lazy dog",
    "The cat chases the mouse"
]
reference_texts = [
    "A fast brown fox leaps over a sleepy canine",
    "A feline pursues a rodent"
]

# Batch calculate scores
bleu_scores = bleu.batch_calculate(generated_texts, reference_texts)
rouge_scores = rouge.batch_calculate(generated_texts, reference_texts)
bert_scores = bert_score.batch_calculate(generated_texts, reference_texts)
```

Furthermore, each metric can be customized during initialization:

```python
# Customize BLEU
bleu = BLEU(n=3)  # Use 3-grams instead of default 4-grams

# Customize ROUGE
rouge = ROUGE(rouge_types=['rouge1', 'rouge2'], use_stemmer=True)

# Customize BERTScore
bert_score = BERTScore(model_type='bert-base-uncased', num_layers=8)
```

Lastly, for advanced users, all metrics support additional parameters:

```python
# Pass additional parameters during calculation
bleu_score = bleu.calculate(text1, text2, additional_params={
    'smoothing_function': custom_smoothing
})
```

## Table of Contents
- [Features](#features)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [TODOs](#todos)
- [Detailed Usage](#detailed-usage)
- [Development](#development)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)
- [Contact](#contact)

## Features

- Implements various metrics for text comparison:
  - N-gram-based metrics (BLEU, ROUGE, JS divergence)
  - Text similarity metrics (Jaccard, Cosine, Levenshtein)
  - Semantic similarity metrics (BERTScore)
- Supports batch processing for efficient computation
- Optimized for different input types (lists, numpy arrays, pandas Series)
- Extendable architecture for easy addition of new metrics
- Testing suite

## Installation

Currently, LLM Metrics is not available on PyPI. To use it, you'll need to clone the repository and set up the environment using Poetry.

1. First, make sure you have Poetry installed. If not, you can install it by following the instructions on the [official Poetry website](https://python-poetry.org/docs/#installation).

2. Clone the repository:
   ```bash
   git clone https://github.com/ai4society/GenAIResultsComparator.git
   cd GenAIResultsComparator
   ```

3. Install the dependencies using Poetry:
   ```bash
   poetry install
   ```

4. Activate the virtual environment:
   ```bash
   poetry shell
   ```

Now you're ready to use LLM Metrics!

## Project Structure

The project structure is as follows:
```shell
.
├── README.md
├── LICENSE
├── .gitignore
├── poetry.lock
├── pyproject.toml
├── .pre-commit-config.yaml
├── .github/
│   └── workflows/
│       └── ci.yml
├── llm_metrics/
│   ├── __init__.py
│   ├── base.py
│   ├── exceptions.py
│   ├── ngram_metrics.py
│   ├── semantic_similarity_metrics.py
│   ├── text_similarity_metrics.py
│   └── utils.py
└── tests/
    ├── __init__.py
    ├── conftest.py
    ├── ngram_metrics_test.py
    ├── semantic_similarity_metrics_test.py
    ├── text_similarity_metrics_test.py
    └── utils_test.py

```

## TODOs
- Add a more comprehensive test suite
- Add more metrics from the [Microsoft article](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics)
- Add support for more input types (e.g., lists, numpy arrays, pandas Series)

## Detailed Usage

### N-gram-based Metrics

#### BLEU (Bilingual Evaluation Understudy)

```python
from llm_metrics import BLEU

bleu = BLEU(n=4)  # n is the maximum n-gram order
score = bleu.calculate(generated_text, reference_text)
```

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

```python
from llm_metrics import ROUGE

rouge = ROUGE(rouge_types=['rouge1', 'rouge2', 'rougeL'])
scores = rouge.calculate(generated_text, reference_text)
```

#### JS Divergence

```python
from llm_metrics import JSDivergence

js_div = JSDivergence()
score = js_div.calculate(generated_text, reference_text)
```

### Text Similarity Metrics

#### Jaccard Similarity

```python
from llm_metrics import JaccardSimilarity

jaccard = JaccardSimilarity()
score = jaccard.calculate(generated_text, reference_text)
```

#### Cosine Similarity

```python
from llm_metrics import CosineSimilarity

cosine = CosineSimilarity()
score = cosine.calculate(generated_text, reference_text)
```

#### Levenshtein Distance

```python
from llm_metrics import LevenshteinDistance

levenshtein = LevenshteinDistance()
distance = levenshtein.calculate(generated_text, reference_text)
```

### Semantic Similarity Metrics

#### BERTScore

```python
from llm_metrics import BERTScore

bert_score = BERTScore(model_type="bert-base-uncased")
score = bert_score.calculate(generated_text, reference_text)
```

### Batch Processing

All metrics support batch processing for efficient computation on multiple texts:

```python
generated_texts = ["Text 1", "Text 2", "Text 3"]
reference_texts = ["Ref 1", "Ref 2", "Ref 3"]

scores = metric.batch_calculate(generated_texts, reference_texts)
```

## Development

To set up the development environment:

1. Clone the repository:
   ```
   git clone https://github.com/ai4society/GenAIResultsComparator.git
   cd GenAIResultsComparator
   ```

2. Install dependencies using Poetry:
   ```
   poetry install
   ```

3. Run tests:
   ```
   poetry run pytest tests/
   ```

### Code Style

We use `pre-commit` hooks to maintain code quality and consistency. The configuration for these hooks is in the `.pre-commit-config.yaml` file. These hooks run automatically on `git commit`, but you can also run them manually:

```
pre-commit run --all-files
```

Our pre-commit hooks include:
- Code formatting with Black
- Import sorting with isort
- Linting with flake8
- Type checking with mypy

### Continuous Integration

We use GitHub Actions for continuous integration. The configuration is in the `.github/workflows/ci.yml` file. This workflow runs on every push and pull request, and includes:

- Running tests on multiple Python versions
- Checking code style and linting
- Building and verifying the package

You can see the status of these checks on the GitHub Actions tab of the repository.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/FeatureName`)
3. Commit your changes (`git commit -m 'Add some FeatureName'`)
4. Push to the branch (`git push origin feature/FeatureName`)
5. Open a Pull Request

Please ensure that your code passes all tests and adheres to our code style guidelines (enforced by pre-commit hooks) before submitting a pull request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- This library uses several open-source packages including NLTK, scikit-learn, and others.
- Special thanks to the creators and maintainers of the implemented metrics.

## Contact

If you have any questions, feel free to reach out to us at [ai4societyteam@gmail.com](mailto:ai4societyteam@gmail.com).
